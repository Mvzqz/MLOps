{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Seoul Bike Sharing â€” EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Perform exploratory data analysis, clean the data, create new features, and save the processed datasets for the modeling phase.\n",
    "\n",
    "**Inputs:**\n",
    "- `seoul_bike_sharing_original.csv` (Reference)\n",
    "- `seoul_bike_sharing_modified.csv` (Main working file)\n",
    "\n",
    "**Outputs:**\n",
    "- `cleaned_modified.csv`\n",
    "- `cleaned_enriched.csv`\n",
    "- `cleaned_enriched_lags.csv`\n",
    "- `cleaned_enriched_lags_plus.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings, json, hashlib, textwrap, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Locations & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming this notebook is in the 'notebooks' directory\n",
    "# Adjust if your structure is different or if using Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Adjust this path to where your data is in Google Drive\n",
    "    p_org = Path(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/Copia de seoul_bike_sharing_original.csv\")\n",
    "    p_mod = Path(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/seoul_bike_sharing_modified.csv\")\n",
    "    output_dir = p_mod.parent\n",
    "except ImportError:\n",
    "    # Local setup\n",
    "    output_dir = Path(\"../data/processed/\")\n",
    "    p_org = Path(\"../data/raw/seoul_bike_sharing_original.csv\")\n",
    "    p_mod = Path(\"../data/raw/seoul_bike_sharing_modified.csv\") # Or wherever the modified one is\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert p_org.exists(), f\"âŒ Original file not found: {p_org}\"\n",
    "assert p_mod.exists(), f\"âŒ Modified file not found: {p_mod}\"\n",
    "\n",
    "org = pd.read_csv(p_org)\n",
    "mod = pd.read_csv(p_mod)\n",
    "\n",
    "print(\"âœ… Files loaded successfully\")\n",
    "print(f\"Original:  {org.shape}\")\n",
    "print(f\"Modified: {mod.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cols(df):\n",
    "    df = df.copy(); df.rename(columns={c: c.strip().replace(\"\\\\xa0\",\" \").replace(\"  \",\" \").strip() for c in df.columns}, inplace=True); return df\n",
    "\n",
    "def add_parsed_date(df):\n",
    "    df = df.copy(); d=[c for c in df.columns if \"date\" in c.lower()]; df[\"__Date\"]=pd.to_datetime(df[d[0]], errors=\"coerce\", dayfirst=True) if d else pd.NaT\n",
    "    h=[c for c in df.columns if \"hour\" in c.lower()]; df[\"__Hour\"]=pd.to_numeric(df[h[0]], errors=\"coerce\") if h else np.nan; return df\n",
    "\n",
    "def guess_target(cols):\n",
    "    for c in cols:\n",
    "        if \"rented\" in c.lower() and \"count\" in c.lower(): return c\n",
    "    return None\n",
    "\n",
    "def clean_df(df, target_col):\n",
    "    df=df.copy()\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[c]=df[c].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "    for c in df.columns:\n",
    "        if c!=target_col and df[c].dtype==object:\n",
    "            num=pd.to_numeric(df[c].str.replace(\",\",\"\").str.replace(\"%\",\"\"), errors=\"coerce\")\n",
    "            if num.notna().sum()>=0.5*len(df): df[c]=num\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        q1,q99=df[c].quantile(0.01), df[c].quantile(0.99)\n",
    "        if pd.notna(q1) and pd.notna(q99) and q99>q1: df[c]=df[c].clip(q1,q99)\n",
    "    return df\n",
    "\n",
    "def overview(df, name):\n",
    "    info = pd.DataFrame({\n",
    "        \"col\": df.columns,\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"n_missing\": df.isna().sum().values,\n",
    "        \"pct_missing\": (100 * df.isna().sum() / len(df)).round(2).values,\n",
    "        \"n_unique\": [df[c].nunique(dropna=True) for c in df.columns]\n",
    "    })\n",
    "    display(info.sort_values(\"pct_missing\", ascending=False).style.set_caption(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize columns and dates\n",
    "org = normalize_cols(org)\n",
    "mod = normalize_cols(mod)\n",
    "org = add_parsed_date(org)\n",
    "mod = add_parsed_date(mod)\n",
    "\n",
    "# Detect target column\n",
    "target_col = guess_target(mod.columns) or guess_target(org.columns)\n",
    "if target_col is None:\n",
    "    n = mod.select_dtypes(include=np.number).columns\n",
    "    target_col = n[0] if len(n) > 0 else mod.columns[0]\n",
    "\n",
    "print(\"ðŸ”¹ Target column detected:\", target_col)\n",
    "\n",
    "# Show overviews\n",
    "overview(mod, \"Modified (pre-cleaning)\")\n",
    "overview(org, \"Original (reference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Target variable analysis ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Basic cleaning of the target column (remove $, commas, spaces)\n",
    "s = mod[target_col].astype(str).str.replace(r\"[,\\s\\$\\â‚¬]\", \"\", regex=True)\n",
    "num = pd.to_numeric(s, errors=\"coerce\")  # convert to numeric, force NaN if text\n",
    "\n",
    "print(f\"Total rows: {len(num)}\")\n",
    "print(f\"Valid values: {num.notna().sum()}  |  Null or non-convertible: {num.isna().sum()}\")\n",
    "\n",
    "# Plot distribution\n",
    "num.dropna().astype(float).plot(kind=\"hist\", bins=30)\n",
    "plt.title(f\"Target Distribution (Modified, pre-cleaning): {target_col}\")\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_clean = clean_df(mod, target_col)\n",
    "mod_clean[target_col] = pd.to_numeric(mod_clean[target_col], errors=\"coerce\")\n",
    "\n",
    "# Drop problematic column if it exists\n",
    "if \"mixed_type_col\" in mod_clean.columns:\n",
    "    mod_clean.drop(columns=[\"mixed_type_col\"], inplace=True)\n",
    "    print(\"Column 'mixed_type_col' dropped.\")\n",
    "\n",
    "mod_clean_path = output_dir / 'cleaned_modified.csv'\n",
    "mod_clean.to_csv(mod_clean_path, index=False)\n",
    "\n",
    "print(\"Saved:\", mod_clean_path)\n",
    "print(\"Final shape ->\", mod_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distribution Comparison (Cleaned vs. Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cols_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (df.columns\n",
    "                  .str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                  .str.replace(\" \", \"_\"))\n",
    "    return df\n",
    "\n",
    "org_norm = normalize_cols_df(org)\n",
    "mod_clean_norm = normalize_cols_df(mod_clean)\n",
    "\n",
    "target_col_norm = guess_target(mod_clean_norm.columns)\n",
    "\n",
    "common = [c for c in mod_clean_norm.columns if c in org_norm.columns and c != target_col_norm]\n",
    "\n",
    "rows = []\n",
    "for c in common:\n",
    "    a = pd.to_numeric(mod_clean_norm[c], errors=\"coerce\").dropna()\n",
    "    b = pd.to_numeric(org_norm[c], errors=\"coerce\").dropna()\n",
    "    if len(a) > 50 and len(b) > 50:\n",
    "        stat, p = ks_2samp(a, b)\n",
    "        rows.append({\n",
    "            \"col\": c,\n",
    "            \"mean_mod\": a.mean(),\n",
    "            \"std_mod\": a.std(),\n",
    "            \"p50_mod\": a.median(),\n",
    "            \"mean_org\": b.mean(),\n",
    "            \"std_org\": b.std(),\n",
    "            \"p50_org\": b.median(),\n",
    "            \"ks_stat\": stat,\n",
    "            \"ks_pvalue\": p\n",
    "        })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows).sort_values(\"ks_stat\", ascending=False)\n",
    "\n",
    "if not cmp_df.empty:\n",
    "    display(cmp_df.style.set_caption(\n",
    "        \"Comparison (cleaned modified vs. original) â€” KS p>0.05 â‰ˆ no strong statistical change\"\n",
    "    ))\n",
    "else:\n",
    "    print(\"âš ï¸ No common numeric columns with enough data to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = pd.read_csv(mod_clean_path)\n",
    "df_enriched.columns = df_enriched.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# --- Date Features ---\n",
    "date_col = next((c for c in df_enriched.columns if \"date\" in c), None)\n",
    "if date_col:\n",
    "    dt = pd.to_datetime(df_enriched[date_col], dayfirst=True, errors='coerce')\n",
    "    df_enriched[\"year\"] = dt.dt.year\n",
    "    df_enriched[\"month\"] = dt.dt.month\n",
    "    df_enriched[\"day\"] = dt.dt.day\n",
    "    df_enriched[\"dayofweek\"] = dt.dt.dayofweek\n",
    "    df_enriched[\"is_weekend\"] = (df_enriched[\"dayofweek\"] >= 5).astype(int)\n",
    "    df_enriched[\"dayofyear\"]  = dt.dt.dayofyear\n",
    "    df_enriched[\"weekofyear\"] = dt.dt.isocalendar().week.astype(\"int64\")\n",
    "    df_enriched[\"quarter\"]    = dt.dt.quarter\n",
    "    df_enriched.drop(columns=[date_col], inplace=True)\n",
    "\n",
    "# --- Cyclical Hour Features ---\n",
    "if \"hour\" in df_enriched.columns:\n",
    "    df_enriched[\"hour_sin\"] = np.sin(2 * np.pi * df_enriched[\"hour\"] / 24)\n",
    "    df_enriched[\"hour_cos\"] = np.cos(2 * np.pi * df_enriched[\"hour\"] / 24)\n",
    "\n",
    "# --- Interaction Features ---\n",
    "if \"temperature(Â°c)\" in df_enriched.columns and \"humidity(%)\" in df_enriched.columns:\n",
    "    df_enriched[\"temp_humidity\"] = df_enriched[\"temperature(Â°c)\"] * df_enriched[\"humidity(%)\"]\n",
    "if \"solar_radiation_(mj/m2)\" in df_enriched.columns and \"wind_speed_(m/s)\" in df_enriched.columns:\n",
    "    df_enriched[\"solar_wind\"] = df_enriched[\"solar_radiation_(mj/m2)\"] * df_enriched[\"wind_speed_(m/s)\"]\n",
    "if \"temperature(Â°c)\" in df_enriched.columns:\n",
    "    df_enriched[\"temp^2\"] = df_enriched[\"temperature(Â°c)\"] ** 2\n",
    "\n",
    "enriched_path = output_dir / 'cleaned_enriched.csv'\n",
    "df_enriched.to_csv(enriched_path, index=False)\n",
    "print(f\"âœ… Enriched dataset saved to: {enriched_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lag & Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags = pd.read_csv(enriched_path)\n",
    "\n",
    "# --- Create Timestamp for sorting ---\n",
    "if all(c in df_lags.columns for c in [\"year\",\"month\",\"day\",\"hour\"]):\n",
    "    df_lags[\"ts\"] = pd.to_datetime(df_lags[[\"year\", \"month\", \"day\", \"hour\"]])\n",
    "    df_lags = df_lags.sort_values(\"ts\").reset_index(drop=True)\n",
    "else:\n",
    "    raise RuntimeError(\"Cannot create timestamp. year, month, day, hour columns are required.\")\n",
    "\n",
    "target_col_norm = guess_target(df_lags.columns)\n",
    "y = df_lags[target_col_norm]\n",
    "\n",
    "# --- Lag Features ---\n",
    "df_lags[\"lag_1h\"]   = y.shift(1)\n",
    "df_lags[\"lag_24h\"]  = y.shift(24)\n",
    "df_lags[\"lag_168h\"] = y.shift(24*7)\n",
    "\n",
    "# --- Rolling Window Features ---\n",
    "df_lags[\"roll_mean_24h\"]  = y.shift(1).rolling(24).mean()\n",
    "df_lags[\"roll_mean_168h\"] = y.shift(1).rolling(24*7).mean()\n",
    "df_lags[\"roll_max_24h\"]   = y.shift(1).rolling(24).max()\n",
    "df_lags[\"roll_min_24h\"]   = y.shift(1).rolling(24).min()\n",
    "\n",
    "if \"temperature(Â°c)\" in df_lags.columns:\n",
    "    df_lags[\"temp_roll_6h\"] = df_lags[\"temperature(Â°c)\"].shift(1).rolling(6).mean()\n",
    "if \"rainfall(mm)\" in df_lags.columns:\n",
    "    df_lags[\"rain_roll_3h\"] = df_lags[\"rainfall(mm)\"].shift(1).rolling(3).mean()\n",
    "\n",
    "# --- Drop rows with NaNs from lags/rolling ---\n",
    "min_warmup = 24*7\n",
    "df_lags_final = df_lags.iloc[min_warmup:].copy()\n",
    "\n",
    "# --- Save final datasets ---\n",
    "lags_path = output_dir / 'cleaned_enriched_lags.csv'\n",
    "df_lags_final.to_csv(lags_path, index=False)\n",
    "print(f\"âœ… Dataset with lags saved to: {lags_path}\")\n",
    "\n",
    "# For compatibility with the original notebook's final step, we save a copy as 'plus'\n",
    "lags_plus_path = output_dir / 'cleaned_enriched_lags_plus.csv'\n",
    "df_lags_final.to_csv(lags_plus_path, index=False)\n",
    "print(f\"âœ… Dataset also saved to: {lags_plus_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
```
```diff