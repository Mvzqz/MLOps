{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22de9c3b",
   "metadata": {
    "id": "22de9c3b"
   },
   "source": [
    "# Seoul Bike Sharing ‚Äî EDA + Limpieza (Modified) y Comparaci√≥n vs Original (Referencia) + Baseline ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf7a68",
   "metadata": {
    "id": "bfbf7a68"
   },
   "source": [
    "**Reglas**\n",
    "- Trabajamos y modelamos con **`seoul_bike_sharing_modified.csv`** (principal).\n",
    "- **`seoul_bike_sharing_original.csv`** es **solo referencia** para validar limpieza/distribuci√≥n.\n",
    "- Ambos CSV en el **mismo directorio** que este notebook en Drive. El notebook puede *auto-descubrir* si no das la ruta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c2d29",
   "metadata": {
    "id": "992c2d29"
   },
   "source": [
    "## 1) Setup e imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c468f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73c468f5",
    "outputId": "fae63207-4332-4a25-8730-a3e09d130451"
   },
   "outputs": [],
   "source": [
    "#@title Setup e imports\n",
    "import os, sys, warnings, json, hashlib, textwrap, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('Versions -> numpy', np.__version__, '| pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6-IUiAbViXg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6-IUiAbViXg",
    "outputId": "f25f419d-91c3-40b4-ebc8-b542cd3b4aa4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68bcee",
   "metadata": {
    "id": "8d68bcee"
   },
   "source": [
    "## 2) Ubicaci√≥n de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31848f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e31848f",
    "outputId": "367ac2b6-c32b-41f3-ccf0-dedaf6309aa9"
   },
   "outputs": [],
   "source": [
    "#@title üîé Diagn√≥stico: buscar el archivo \"modified\" en todo tu Drive\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "bases = [Path('/content/drive/MyDrive')]\n",
    "shared_root = Path('/content/drive/Shared drives')\n",
    "if shared_root.exists():\n",
    "    bases.append(shared_root)\n",
    "\n",
    "patterns = [\n",
    "    '*seoul*modified*',\n",
    "    '*seoul*bike*mod*',\n",
    "    '*modified*seoul*',\n",
    "    '*bike*sharing*mod*',\n",
    "]\n",
    "\n",
    "hits = []\n",
    "for base in bases:\n",
    "    for pat in patterns:\n",
    "        hits.extend(p for p in base.rglob(pat) if p.suffix.lower() in {'.csv', '.xlsx'})\n",
    "\n",
    "seen = set(); unique = []\n",
    "for p in hits:\n",
    "    if str(p) not in seen:\n",
    "        seen.add(str(p)); unique.append(p)\n",
    "\n",
    "print(f'Encontrados: {len(unique)}')\n",
    "for i, p in enumerate(unique[:200]):\n",
    "    print(f'[{i}] {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C8ULfcpPZBp5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8ULfcpPZBp5",
    "outputId": "b47ffd18-b4b3-4293-bd11-033a1e245f84"
   },
   "outputs": [],
   "source": [
    "#@title üß© Usar el mismo archivo para original y modificado\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta del √∫nico archivo que tienes (ajusta si tu carpeta cambia)\n",
    "p_org = Path(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/Copia de seoul_bike_sharing_original.csv\")\n",
    "p_mod = p_org  # Usa el mismo archivo temporalmente\n",
    "\n",
    "assert p_org.exists(), f\"‚ùå No existe: {p_org}\"\n",
    "print(\"‚úÖ Archivos listos (ambos apuntan al original):\")\n",
    "print(\"Original:\", p_org)\n",
    "print(\"Modified:\", p_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3S56NetSawaG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "3S56NetSawaG",
    "outputId": "4ac25b7d-f530-47ff-d3f5-a94859b6e78d"
   },
   "outputs": [],
   "source": [
    "#@title 1Ô∏è‚É£ Leer el dataset original\n",
    "import pandas as pd\n",
    "\n",
    "df_org = pd.read_csv(p_org)\n",
    "df_mod = pd.read_csv(p_mod)\n",
    "\n",
    "print(\"‚úÖ Archivos cargados correctamente\")\n",
    "print(\"Dimensiones del dataset:\", df_org.shape)\n",
    "df_org.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CbPetDQta1ic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbPetDQta1ic",
    "outputId": "8a24b3b7-3a24-4aef-dd66-77401562ecac"
   },
   "outputs": [],
   "source": [
    "#@title 2Ô∏è‚É£ Crear versi√≥n modificada en la misma carpeta\n",
    "import pandas as pd\n",
    "\n",
    "df_mod = df_org.copy()\n",
    "\n",
    "# Ejemplo de modificaci√≥n: eliminar 10 primeras filas\n",
    "df_mod = df_mod.iloc[10:, :]\n",
    "\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/seoul_bike_sharing_modified.csv\"\n",
    "df_mod.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Archivo modificado creado en Drive:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b-ayhwZbCMK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b-ayhwZbCMK",
    "outputId": "4f1f9e1c-20e6-467f-8ce1-3638029bb5bc"
   },
   "outputs": [],
   "source": [
    "#@title 1Ô∏è‚É£ Actualizar ruta al archivo modificado\n",
    "from pathlib import Path\n",
    "\n",
    "p_mod = Path(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/seoul_bike_sharing_modified.csv\")\n",
    "\n",
    "assert p_mod.exists(), f\"‚ùå No existe el archivo modificado: {p_mod}\"\n",
    "print(\"‚úÖ Archivo modificado detectado correctamente:\")\n",
    "print(p_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RbV6A1J-bFhn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "RbV6A1J-bFhn",
    "outputId": "04762dfc-4979-42e0-c306-f615ea6dc723"
   },
   "outputs": [],
   "source": [
    "#@title 2Ô∏è‚É£ Leer ambos archivos (original y modificado)\n",
    "import pandas as pd\n",
    "\n",
    "df_org = pd.read_csv(p_org)\n",
    "df_mod = pd.read_csv(p_mod)\n",
    "\n",
    "print(\"‚úÖ Archivos cargados correctamente\")\n",
    "print(f\"Original:  {df_org.shape} filas x columnas\")\n",
    "print(f\"Modificado: {df_mod.shape} filas x columnas\")\n",
    "\n",
    "display(df_org.head(2))\n",
    "display(df_mod.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7t3yOdubK0o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7t3yOdubK0o",
    "outputId": "ae9b4167-7e11-4812-e056-851f5e86cbde"
   },
   "outputs": [],
   "source": [
    "#@title 3Ô∏è‚É£ Comparar diferencias b√°sicas\n",
    "diff_rows = len(df_org) - len(df_mod)\n",
    "print(f\"El dataset modificado tiene {diff_rows} filas menos que el original.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580e305",
   "metadata": {
    "id": "f580e305"
   },
   "source": [
    "## 3) Carga (principal=modified, referencia=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943e63e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d943e63e",
    "outputId": "cb92d278-c12d-49ee-bbd1-f5470d1ca6d0"
   },
   "outputs": [],
   "source": [
    "#@title Cargar\n",
    "org = pd.read_csv(p_org); mod = pd.read_csv(p_mod)\n",
    "org.head(2), mod.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hx1WcCM0JkTu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "hx1WcCM0JkTu",
    "outputId": "d02943ce-2edc-40b4-bf9f-b530f09eab1b"
   },
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ApHv1WqGJm_E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "ApHv1WqGJm_E",
    "outputId": "c6937e06-30e1-4795-dc7a-2a23f63abb6a"
   },
   "outputs": [],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9c22",
   "metadata": {
    "id": "b6fe9c22"
   },
   "source": [
    "## 4) Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211a812",
   "metadata": {
    "id": "9211a812"
   },
   "outputs": [],
   "source": [
    "#@title Funciones\n",
    "def normalize_cols(df):\n",
    "    df = df.copy(); df.rename(columns={c: c.strip().replace(\"\\\\xa0\",\" \").replace(\"  \",\" \").strip() for c in df.columns}, inplace=True); return df\n",
    "def add_parsed_date(df):\n",
    "    df = df.copy(); d=[c for c in df.columns if \"date\" in c.lower()]; df[\"__Date\"]=pd.to_datetime(df[d[0]], errors=\"coerce\", dayfirst=True) if d else pd.NaT\n",
    "    h=[c for c in df.columns if \"hour\" in c.lower()]; df[\"__Hour\"]=pd.to_numeric(df[h[0]], errors=\"coerce\") if h else np.nan; return df\n",
    "def guess_target(cols):\n",
    "    for c in cols:\n",
    "        if \"rented\" in c.lower() and \"count\" in c.lower(): return c\n",
    "    return None\n",
    "def clean_df(df, target_col):\n",
    "    df=df.copy()\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[c]=df[c].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "    for c in df.columns:\n",
    "        if c!=target_col and df[c].dtype==object:\n",
    "            num=pd.to_numeric(df[c].str.replace(\",\",\"\").str.replace(\"%\",\"\"), errors=\"coerce\")\n",
    "            if num.notna().sum()>=0.5*len(df): df[c]=num\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        q1,q99=df[c].quantile(0.01), df[c].quantile(0.99)\n",
    "        if pd.notna(q1) and pd.notna(q99) and q99>q1: df[c]=df[c].clip(q1,q99)\n",
    "    return df\n",
    "def detect_categoricals(df, target_col):\n",
    "    cats=list(df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns)\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[c].nunique(dropna=True)<=20 and c!=target_col: cats.append(c)\n",
    "    return sorted([c for c in set(cats) if not c.startswith(\"__\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a473f97",
   "metadata": {
    "id": "7a473f97"
   },
   "source": [
    "## 5) Objetivo y EDA r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd947dfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dd947dfe",
    "outputId": "98ec5d82-f70f-49d3-9ddc-6eea552e0ff1"
   },
   "outputs": [],
   "source": [
    "#@title üéØ Objetivo y EDA\n",
    "\n",
    "# Normalizaci√≥n de columnas y fechas\n",
    "org = normalize_cols(org)\n",
    "mod = normalize_cols(mod)\n",
    "org = add_parsed_date(org)\n",
    "mod = add_parsed_date(mod)\n",
    "\n",
    "# Detecci√≥n autom√°tica de columna objetivo\n",
    "target_col = guess_target(mod.columns) or guess_target(org.columns)\n",
    "if target_col is None:\n",
    "    n = mod.select_dtypes(include=np.number).columns\n",
    "    target_col = n[0] if len(n) > 0 else mod.columns[0]\n",
    "\n",
    "print(\"üîπ Columna objetivo detectada:\", target_col)\n",
    "\n",
    "# --- Funci√≥n de resumen (overview) ---\n",
    "def overview(df, name):\n",
    "    info = pd.DataFrame({\n",
    "        \"col\": df.columns,\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"n_missing\": df.isna().sum().values,\n",
    "        \"pct_missing\": (100 * df.isna().sum() / len(df)).round(2).values,\n",
    "        \"n_unique\": [df[c].nunique(dropna=True) for c in df.columns]\n",
    "    })\n",
    "    display(info.sort_values(\"pct_missing\", ascending=False).style.set_caption(name))\n",
    "\n",
    "# Mostrar res√∫menes de ambos conjuntos\n",
    "overview(mod, \"Modified (pre-limpieza)\")\n",
    "overview(org, \"Original (referencia)\")\n",
    "\n",
    "# --- An√°lisis de la variable objetivo ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Limpieza b√°sica de la columna objetivo (quita $, comas, espacios)\n",
    "s = mod[target_col].astype(str).str.replace(r\"[,\\s\\$\\‚Ç¨]\", \"\", regex=True)\n",
    "num = pd.to_numeric(s, errors=\"coerce\")  # convierte a num√©rico, fuerza NaN si hay texto\n",
    "\n",
    "print(f\"Total de filas: {len(num)}\")\n",
    "print(f\"Valores v√°lidos: {num.notna().sum()}  |  Nulos o no convertibles: {num.isna().sum()}\")\n",
    "\n",
    "# Graficar distribuci√≥n\n",
    "num.dropna().astype(float).plot(kind=\"hist\", bins=30)\n",
    "plt.title(f\"Distribuci√≥n del objetivo (Modified, pre-limpieza): {target_col}\")\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z-GgX4IrbpPl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-GgX4IrbpPl",
    "outputId": "2a70c5d0-a31b-4fb8-9a44-c8ee75dece01"
   },
   "outputs": [],
   "source": [
    "# Usaremos df_mod como \"mod\" para que tu celda funcione\n",
    "mod = df_mod.copy()\n",
    "\n",
    "# Define el target del dataset de Seoul Bike\n",
    "target_col = 'Rented Bike Count'  # <- este s√≠ existe en tu CSV\n",
    "\n",
    "print(\"Columnas disponibles en 'mod':\")\n",
    "print(list(mod.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q_6sB5GBGH1x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_6sB5GBGH1x",
    "outputId": "507b2b08-806a-4147-b70c-ee27cfba0f33"
   },
   "outputs": [],
   "source": [
    "#@title üîç An√°lisis forense de una columna categ√≥rica (listo para correr)\n",
    "# Cambia aqu√≠ la columna a analizar:\n",
    "COL = 'Seasons'  # <-- edita: 'Seasons' | 'Holiday' | 'Functioning Day' | etc.\n",
    "\n",
    "import numpy as np, pandas as pd, re, warnings\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== 0) Preparaci√≥n de datos =====\n",
    "# Detecta df_mod / df_org y define 'mod' de trabajo\n",
    "if 'df_mod' in globals():\n",
    "    mod = df_mod.copy()\n",
    "elif 'df_org' in globals():\n",
    "    mod = df_org.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"No encuentro 'df_mod' ni 'df_org'. Carga primero tu dataset con pandas.\")\n",
    "\n",
    "# Target t√≠pico del dataset de Seoul Bike (ajusta si usas otro):\n",
    "target_col = 'Rented Bike Count'\n",
    "if target_col not in mod.columns:\n",
    "    raise KeyError(f\"No encuentro la columna target '{target_col}' en el DataFrame.\")\n",
    "\n",
    "# Sugerencias si la columna elegida no existe\n",
    "if COL not in mod.columns:\n",
    "    obj_cols = [c for c in mod.columns if mod[c].dtype == 'object']\n",
    "    mixed_type_cols = [c for c in mod.columns if mod[c].map(type).nunique() > 1]\n",
    "    msg = [\n",
    "        f\"‚ùå La columna '{COL}' no existe.\",\n",
    "        \"üëâ Columnas disponibles (primeras 25): \" + \", \".join(list(mod.columns)[:25]),\n",
    "        \"üí° Categ√≥ricas (object): \" + (\", \".join(obj_cols) if obj_cols else \"(ninguna)\"),\n",
    "        \"üß™ Posibles 'mixtas' (tipos mezclados): \" + (\", \".join(mixed_type_cols) if mixed_type_cols else \"(ninguna)\"),\n",
    "    ]\n",
    "    raise KeyError(\"\\n\".join(msg))\n",
    "\n",
    "print(f\"=== An√°lisis de: `{COL}` ===\")\n",
    "\n",
    "# ===== 1) Perfil b√°sico =====\n",
    "s = mod[COL]\n",
    "print(\"dtype:\", s.dtype)\n",
    "n = len(s)\n",
    "n_missing = s.isna().sum()\n",
    "n_unique = s.nunique(dropna=True)\n",
    "print(f\"filas={n:,} | n_missing={n_missing:,} ({100*n_missing/n:.2f}%) | n_unique={n_unique:,} ({100*n_unique/n:.2f}% del total)\")\n",
    "\n",
    "types_counts = Counter(type(x).__name__ for x in s)\n",
    "print(\"Tipos de valores observados:\", dict(types_counts))\n",
    "\n",
    "top_vals = pd.Series(s.astype(str)).value_counts(dropna=False).head(12)\n",
    "print(\"\\nTop 12 valores (crudos):\")\n",
    "print(top_vals)\n",
    "\n",
    "# ===== 2) Convertibilidad a num√©rico =====\n",
    "print(\"\\n=== 2) ¬øSe puede convertir a num√©rico? ===\")\n",
    "sn = (\n",
    "    s.astype(str)\n",
    "     .str.strip()\n",
    "     .str.replace(r\"[,\\s%]\", \"\", regex=True)\n",
    "     .str.replace(r\"[^\\-\\.\\dEe+]\", \"\", regex=True)\n",
    ")\n",
    "num = pd.to_numeric(sn, errors='coerce')\n",
    "pct_numeric = 100 * num.notna().mean()\n",
    "print(f\"% convertibles a n√∫mero: {pct_numeric:.2f}%\")\n",
    "if num.notna().sum() > 0:\n",
    "    print(\"Resumen (solo convertibles):\")\n",
    "    print(num.dropna().astype(float).describe())\n",
    "\n",
    "# ===== 3) Asociaci√≥n con el target =====\n",
    "print(\"\\n=== 3) Asociaci√≥n con el target ===\")\n",
    "y = pd.to_numeric(mod[target_col], errors='coerce')\n",
    "mask = y.notna()\n",
    "y_valid = y[mask]\n",
    "\n",
    "if num.notna().sum() > 0:\n",
    "    corr = np.corrcoef(num[mask].fillna(num[mask].median()), y_valid)[0,1]\n",
    "    print(f\"Correlaci√≥n num√©rica con {target_col}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"No hay suficientes valores num√©ricos para correlaci√≥n directa.\")\n",
    "\n",
    "if s.nunique(dropna=True) <= 50:\n",
    "    grp = pd.DataFrame({\"col\": s[mask], \"y\": y_valid}).dropna().groupby(\"col\")[\"y\"].mean().sort_values(ascending=False)\n",
    "    print(\"\\nMedia del target por categor√≠a (top 10):\")\n",
    "    print(grp.head(10))\n",
    "\n",
    "# ===== 4) ¬øPredice demasiado bien por s√≠ sola? =====\n",
    "print(\"\\n=== 4) ¬øPredice demasiado bien por s√≠ sola? ===\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_tmp = mod[[COL, target_col]].copy()\n",
    "df_tmp[target_col] = pd.to_numeric(df_tmp[target_col], errors='coerce')\n",
    "df_tmp = df_tmp.dropna(subset=[target_col]).copy()\n",
    "X_one = df_tmp[[COL]]\n",
    "y_one = df_tmp[target_col].astype(float)\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), [COL])\n",
    "])\n",
    "model_one = Pipeline(steps=[\n",
    "    ('enc', preproc),\n",
    "    ('rf', RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_r2 = cross_val_score(model_one, X_one, y_one, scoring='r2', cv=cv)\n",
    "scores_rmse = -cross_val_score(model_one, X_one, y_one, scoring='neg_root_mean_squared_error', cv=cv)\n",
    "\n",
    "print(f\"CV R¬≤ (solo `{COL}`):  mean={scores_r2.mean():.4f} ¬±{scores_r2.std():.4f}\")\n",
    "print(f\"CV RMSE (solo `{COL}`): mean={scores_rmse.mean():.2f} ¬±{scores_rmse.std():.2f}\")\n",
    "\n",
    "# ===== 5) Comparaci√≥n con y sin la columna =====\n",
    "print(\"\\n=== 5) Comparaci√≥n con y sin la columna ===\")\n",
    "mod_numish = mod.copy()\n",
    "for c in mod_numish.select_dtypes('object').columns:\n",
    "    coerced = pd.to_numeric(mod_numish[c].astype(str).str.replace(r\"[,\\s%]\", \"\", regex=True), errors='coerce')\n",
    "    if coerced.notna().mean() >= 0.6:\n",
    "        mod_numish[c] = coerced\n",
    "\n",
    "A = mod_numish.dropna(subset=[target_col]).copy()\n",
    "A_y = pd.to_numeric(A[target_col], errors='coerce')\n",
    "A = A.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "A = A.loc[A_y.notna()]\n",
    "A_y = A_y.loc[A_y.notna()].astype(float)\n",
    "\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(A, A_y, test_size=0.2, random_state=42)\n",
    "rf_A = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_A.fit(X_train_A, y_train_A)\n",
    "r2_A = r2_score(y_test_A, rf_A.predict(X_test_A))\n",
    "print(f\"R¬≤ (CON `{COL}`): {r2_A:.4f}\")\n",
    "\n",
    "B = mod_numish.drop(columns=[COL], errors='ignore')\n",
    "B = B.dropna(subset=[target_col]).copy()\n",
    "B_y = pd.to_numeric(B[target_col], errors='coerce')\n",
    "B = B.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "B = B.loc[B_y.notna()]\n",
    "B_y = B_y.loc[B_y.notna()].astype(float)\n",
    "\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(B, B_y, test_size=0.2, random_state=42)\n",
    "rf_B = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_B.fit(X_train_B, y_train_B)\n",
    "r2_B = r2_score(y_test_B, rf_B.predict(X_test_B))\n",
    "print(f\"R¬≤ (SIN `{COL}`): {r2_B:.4f}\")\n",
    "\n",
    "# ===== 6) Se√±ales emp√≠ricas + 7) Recomendaci√≥n =====\n",
    "print(\"\\n=== 6) Se√±ales emp√≠ricas ===\")\n",
    "flags = []\n",
    "if n_unique / n > 0.9:\n",
    "    flags.append(\"cardinalidad_altisima\")\n",
    "if len(types_counts) > 1:\n",
    "    flags.append(\"tipos_mezclados\")\n",
    "if scores_r2.mean() >= 0.2:\n",
    "    flags.append(\"predice_demasiado_bien_sola\")\n",
    "if (r2_A - r2_B > 0.05):\n",
    "    flags.append(\"impacto_excesivo_en_modelo\")\n",
    "\n",
    "print(\"Flags activadas:\", flags if flags else \"ninguna\")\n",
    "\n",
    "print(\"\\n=== 7) Recomendaci√≥n preliminar ===\")\n",
    "if flags:\n",
    "    print(f\"‚Üí Sugerencia: revisar/posible eliminar `{COL}` del modelado. Motivos:\")\n",
    "    if \"cardinalidad_altisima\" in flags: print(\"  - Cardinalidad ~√∫nica por fila (parece ID o hash).\")\n",
    "    if \"tipos_mezclados\" in flags: print(\"  - Mezcla de tipos (strings/n√∫meros/NaN).\")\n",
    "    if \"predice_demasiado_bien_sola\" in flags: print(\"  - Predice mucho por s√≠ sola; validar CV temporal para evitar sobreajuste estacional.\")\n",
    "    if \"impacto_excesivo_en_modelo\" in flags: print(\"  - Aporta ganancia an√≥mala al R¬≤; validar importancia y fuga potencial.\")\n",
    "else:\n",
    "    print(f\"‚Üí Mantener `{COL}` con cautela; no hay se√±ales fuertes de fuga. Validar con CV y splits temporales.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-US-7CEpij_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "c-US-7CEpij_",
    "outputId": "692bca41-d4a9-459f-e613-f0a673a9f534"
   },
   "outputs": [],
   "source": [
    "#@title üîé Benchmark de TODAS las columnas categ√≥ricas (resumen y flags)\n",
    "import pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Preparaci√≥n\n",
    "if 'df_mod' in globals():\n",
    "    base_df = df_mod.copy()\n",
    "elif 'df_org' in globals():\n",
    "    base_df = df_org.copy()\n",
    "else:\n",
    "    raise RuntimeError(\"No encuentro df_mod ni df_org. Carga el dataset primero.\")\n",
    "\n",
    "target_col = 'Rented Bike Count'\n",
    "assert target_col in base_df.columns, f\"No est√° {target_col} en el DataFrame.\"\n",
    "\n",
    "# Copia para coerci√≥n num√©rica ligera\n",
    "def coerce_numish(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes('object').columns:\n",
    "        cc = pd.to_numeric(df[c].astype(str).str.replace(r\"[,\\s%]\", \"\", regex=True), errors='coerce')\n",
    "        if cc.notna().mean() >= 0.6:\n",
    "            df[c] = cc\n",
    "    return df\n",
    "\n",
    "mod = base_df.copy()\n",
    "mod_numish = coerce_numish(mod)\n",
    "\n",
    "# Lista de candidatas categ√≥ricas\n",
    "cat_cols = [c for c in mod.columns if mod[c].dtype == 'object' and c != target_col]\n",
    "results = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for COL in cat_cols:\n",
    "    s = mod[COL]\n",
    "    n = len(s)\n",
    "    n_missing = s.isna().sum()\n",
    "    n_unique = s.nunique(dropna=True)\n",
    "    types_counts = Counter(type(x).__name__ for x in s)\n",
    "\n",
    "    # --- Modelo SOLO con COL ---\n",
    "    df_tmp = mod[[COL, target_col]].copy()\n",
    "    df_tmp[target_col] = pd.to_numeric(df_tmp[target_col], errors='coerce')\n",
    "    df_tmp = df_tmp.dropna(subset=[target_col]).copy()\n",
    "    X_one = df_tmp[[COL]]\n",
    "    y_one = df_tmp[target_col].astype(float)\n",
    "\n",
    "    preproc = ColumnTransformer([\n",
    "        ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), [COL])\n",
    "    ])\n",
    "    model_one = Pipeline(steps=[\n",
    "        ('enc', preproc),\n",
    "        ('rf', RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        scores_r2 = cross_val_score(model_one, X_one, y_one, scoring='r2', cv=kf)\n",
    "        scores_rmse = -cross_val_score(model_one, X_one, y_one, scoring='neg_root_mean_squared_error', cv=kf)\n",
    "        r2_solo = scores_r2.mean()\n",
    "        rmse_solo = scores_rmse.mean()\n",
    "    except Exception as e:\n",
    "        r2_solo, rmse_solo = np.nan, np.nan\n",
    "\n",
    "    # --- Modelo global CON la columna ---\n",
    "    A = mod_numish.dropna(subset=[target_col]).copy()\n",
    "    A_y = pd.to_numeric(A[target_col], errors='coerce')\n",
    "    A = A.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "    A = A.loc[A_y.notna()]\n",
    "    A_y = A_y.loc[A_y.notna()].astype(float)\n",
    "\n",
    "    if A.shape[1] >= 1:\n",
    "        X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(A, A_y, test_size=0.2, random_state=42)\n",
    "        rf_A = RandomForestRegressor(n_estimators=200, random_state=42).fit(X_train_A, y_train_A)\n",
    "        r2_A = r2_score(y_test_A, rf_A.predict(X_test_A))\n",
    "    else:\n",
    "        r2_A = np.nan\n",
    "\n",
    "    # --- Modelo global SIN la columna (si es num√©rica no afecta; si es object s√≠ la quita) ---\n",
    "    B = mod_numish.drop(columns=[COL], errors='ignore')\n",
    "    B = B.dropna(subset=[target_col]).copy()\n",
    "    B_y = pd.to_numeric(B[target_col], errors='coerce')\n",
    "    B = B.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "    B = B.loc[B_y.notna()]\n",
    "    B_y = B_y.loc[B_y.notna()].astype(float)\n",
    "\n",
    "    if B.shape[1] >= 1:\n",
    "        X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(B, B_y, test_size=0.2, random_state=42)\n",
    "        rf_B = RandomForestRegressor(n_estimators=200, random_state=42).fit(X_train_B, y_train_B)\n",
    "        r2_B = r2_score(y_test_B, rf_B.predict(X_test_B))\n",
    "    else:\n",
    "        r2_B = np.nan\n",
    "\n",
    "    delta_r2 = (r2_A - r2_B) if (pd.notna(r2_A) and pd.notna(r2_B)) else np.nan\n",
    "\n",
    "    # --- Flags (mismas reglas que tu celda) ---\n",
    "    flags = []\n",
    "    if n_unique / n > 0.9:\n",
    "        flags.append(\"cardinalidad_altisima\")\n",
    "    if len(types_counts) > 1:\n",
    "        flags.append(\"tipos_mezclados\")\n",
    "    if pd.notna(r2_solo) and r2_solo >= 0.20:\n",
    "        flags.append(\"predice_demasiado_bien_sola\")\n",
    "    if pd.notna(delta_r2) and delta_r2 > 0.05:\n",
    "        flags.append(\"impacto_excesivo_en_modelo\")\n",
    "\n",
    "    results.append({\n",
    "        \"col\": COL,\n",
    "        \"dtype\": str(s.dtype),\n",
    "        \"filas\": n,\n",
    "        \"n_missing\": int(n_missing),\n",
    "        \"pct_missing\": round(100*n_missing/n, 2),\n",
    "        \"n_unique\": int(n_unique),\n",
    "        \"cardinalidad\": round(n_unique/n, 4),\n",
    "        \"tipos_observados\": dict(types_counts),\n",
    "        \"CV_R2_sola\": round(r2_solo, 4) if pd.notna(r2_solo) else np.nan,\n",
    "        \"CV_RMSE_sola\": round(rmse_solo, 2) if pd.notna(rmse_solo) else np.nan,\n",
    "        \"R2_con_col\": round(r2_A, 4) if pd.notna(r2_A) else np.nan,\n",
    "        \"R2_sin_col\": round(r2_B, 4) if pd.notna(r2_B) else np.nan,\n",
    "        \"delta_R2\": round(delta_r2, 4) if pd.notna(delta_r2) else np.nan,\n",
    "        \"flags\": \", \".join(flags) if flags else \"\",\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(results).sort_values(\n",
    "    by=[\"delta_R2\", \"CV_R2_sola\", \"cardinalidad\"], ascending=[False, False, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Resumen de columnas categ√≥ricas analizadas:\")\n",
    "display(summary.head(10))\n",
    "\n",
    "# Guardar a Drive (misma carpeta del dataset)\n",
    "out_path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/summary_categoricas.csv\"\n",
    "summary.to_csv(out_path, index=False)\n",
    "print(\"üíæ Guardado en:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wZVAzuA_s6Fv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "wZVAzuA_s6Fv",
    "outputId": "d2968569-baca-49b1-cdf5-e8bbe518dee7"
   },
   "outputs": [],
   "source": [
    "#@title üïí Parseo robusto de fechas y variables temporales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "assert 'mod' in globals(), \"No encuentro el DataFrame 'mod'. Asegura que ya cargaste df_org/df_mod y definiste 'mod'.\"\n",
    "\n",
    "col = 'Date'\n",
    "assert col in mod.columns, f\"No existe la columna '{col}' en mod.\"\n",
    "\n",
    "# Normaliza separadores y limpia espacios\n",
    "s = mod[col].astype(str).str.strip().str.replace(r'[\\.\\-]', '/', regex=True)\n",
    "\n",
    "# 1) Intento principal: formatos mixtos con d√≠a primero\n",
    "dt = pd.to_datetime(s, format='mixed', dayfirst=True, errors='coerce')\n",
    "\n",
    "# 2) Reintentos para lo que quede sin parsear (algunos datasets traen %Y/%m/%d)\n",
    "mask = dt.isna()\n",
    "if mask.any():\n",
    "    dt.loc[mask] = pd.to_datetime(s[mask], format='%Y/%m/%d', errors='coerce')\n",
    "mask = dt.isna()\n",
    "if mask.any():\n",
    "    dt.loc[mask] = pd.to_datetime(s[mask], format='%d/%m/%Y', errors='coerce')\n",
    "mask = dt.isna()\n",
    "\n",
    "# Diagn√≥stico r√°pido\n",
    "bad = int(mask.sum())\n",
    "print(f\"‚úÖ Parseadas: {len(dt)-bad:,}  |  ‚ùå Sin parsear: {bad:,}\")\n",
    "if bad:\n",
    "    # Muestra ejemplos problem√°ticos\n",
    "    print(\"Ejemplos sin parsear:\", s[mask].unique()[:5])\n",
    "\n",
    "# Si todo sali√≥ bien, seguimos\n",
    "assert dt.notna().mean() > 0.98, \"Demasiadas fechas sin parsear. Revisa el tipo de valores impresos arriba.\"\n",
    "\n",
    "# Asigna y crea variables temporales\n",
    "mod[col] = dt\n",
    "mod['year']       = dt.dt.year\n",
    "mod['month']      = dt.dt.month\n",
    "mod['dayofweek']  = dt.dt.dayofweek  # 0=Lunes\n",
    "mod['is_weekend'] = mod['dayofweek'].isin([5,6]).astype(int)\n",
    "mod['day']        = dt.dt.day\n",
    "\n",
    "print(\"Variables creadas:\", ['year','month','dayofweek','is_weekend','day'])\n",
    "mod.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OrICnSmLt_9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrICnSmLt_9a",
    "outputId": "1e7b5d8e-4f70-4de3-ab89-10d116d5db15"
   },
   "outputs": [],
   "source": [
    "#@title üßπ Eliminar 'Date' cruda y preparar X, y\n",
    "target_col = \"Rented Bike Count\"\n",
    "mod = mod.drop(columns=[\"Date\"], errors=\"ignore\")\n",
    "X = mod.drop(columns=[target_col], errors=\"ignore\")\n",
    "y = mod[target_col]\n",
    "print(\"Shape X:\", X.shape, \"| Shape y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OEHidbsVu6q7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "OEHidbsVu6q7",
    "outputId": "f4c8eec7-a943-4c54-9acb-458cd1522e48"
   },
   "outputs": [],
   "source": [
    "#@title üìÜ Validaci√≥n temporal con preprocesamiento (OneHot + Imputaci√≥n)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Asegura que ya tienes 'mod' con las columnas temporales creadas\n",
    "assert 'mod' in globals(), \"No encuentro el DataFrame 'mod'. Corre primero la celda que crea year/month/dayofweek/etc.\"\n",
    "target_col = \"Rented Bike Count\"\n",
    "assert target_col in mod.columns, f\"No existe '{target_col}' en mod.\"\n",
    "\n",
    "# 1) Orden temporal\n",
    "mod_sorted = mod.sort_values(['year','month','day']).reset_index(drop=True)\n",
    "\n",
    "# 2) X, y y selecci√≥n de tipos\n",
    "X_sorted = mod_sorted.drop(columns=[target_col])\n",
    "y_sorted = mod_sorted[target_col]\n",
    "\n",
    "cat_cols = X_sorted.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X_sorted.columns.difference(cat_cols).tolist()\n",
    "\n",
    "print(\"Categ√≥ricas:\", cat_cols)\n",
    "print(\"Num√©ricas:\", num_cols[:10], \" ...\")\n",
    "\n",
    "# 3) Preprocesamiento: imputaci√≥n + one-hot para categ√≥ricas\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imp', SimpleImputer(strategy='most_frequent')),\n",
    "            ('oh', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 4) Modelo en pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('rf', RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# 5) Validaci√≥n temporal (sin fuga)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scores = cross_val_score(model, X_sorted, y_sorted, cv=tscv, scoring='r2')\n",
    "print(\"‚úÖ R¬≤ promedio (TimeSeriesSplit):\", round(np.mean(scores), 4))\n",
    "print(\"   Std:\", round(np.std(scores), 4))\n",
    "\n",
    "# 6) Entrenamiento final con split temporal 80/20 y residuales\n",
    "split_idx = int(len(mod_sorted)*0.8)\n",
    "X_train, y_train = X_sorted.iloc[:split_idx], y_sorted.iloc[:split_idx]\n",
    "X_test,  y_test  = X_sorted.iloc[split_idx:], y_sorted.iloc[split_idx:]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Calcular m√©tricas manualmente (compatible con versiones viejas)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# RMSE = ra√≠z cuadrada del MSE manualmente\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\n‚úÖ R¬≤ (test temporal): {r2:.4f}\")\n",
    "print(f\"üìâ RMSE: {rmse:.2f}\")\n",
    "\n",
    "# --- Gr√°fica de residuales\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_test, y_pred - y_test, alpha=0.5)\n",
    "plt.axhline(0, linestyle='--', color='red')\n",
    "plt.xlabel(\"Rented Bike Count (real)\")\n",
    "plt.ylabel(\"Residual (pred - real)\")\n",
    "plt.title(\"Residuales (split temporal 80/20)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfzO1tsowiaD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xfzO1tsowiaD",
    "outputId": "18f7d2b0-7a18-4d4a-db5e-63c3e22ee18e"
   },
   "outputs": [],
   "source": [
    "#@title üåü Importancia de las variables (feature importance)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extraer importancias desde el modelo dentro del pipeline\n",
    "rf_model = model.named_steps['rf']\n",
    "encoder = model.named_steps['prep']\n",
    "\n",
    "# Obtener nombres de columnas transformadas\n",
    "cat_cols = encoder.transformers_[1][2]\n",
    "num_cols = encoder.transformers_[0][2]\n",
    "oh_names = list(encoder.named_transformers_['cat'].named_steps['oh'].get_feature_names_out(cat_cols))\n",
    "all_features = num_cols + oh_names\n",
    "\n",
    "# Crear DataFrame con importancias\n",
    "feat_imp = pd.DataFrame({\n",
    "    'Variable': all_features,\n",
    "    'Importancia': rf_model.feature_importances_\n",
    "}).sort_values('Importancia', ascending=False)\n",
    "\n",
    "# Mostrar top 15\n",
    "print(\"üìä Top 15 variables m√°s importantes:\")\n",
    "display(feat_imp.head(15))\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(feat_imp.head(15)['Variable'][::-1], feat_imp.head(15)['Importancia'][::-1])\n",
    "plt.title(\"Importancia de las variables (Random Forest)\")\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf7374",
   "metadata": {
    "id": "1fbf7374"
   },
   "source": [
    "## 6) Limpieza SOLO en modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481b8ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d481b8ee",
    "outputId": "f6030785-42e9-4864-e521-9ad1afcb04af"
   },
   "outputs": [],
   "source": [
    "#@title Limpieza y guardado\n",
    "mod_clean = clean_df(mod, target_col)\n",
    "mod_clean[target_col] = pd.to_numeric(mod_clean[target_col], errors=\"coerce\")\n",
    "\n",
    "# Eliminar columna problem√°tica si existe\n",
    "if \"mixed_type_col\" in mod_clean.columns:\n",
    "    mod_clean.drop(columns=[\"mixed_type_col\"], inplace=True)\n",
    "    print(\"Columna 'mixed_type_col' eliminada.\")\n",
    "\n",
    "# Guardar solo el dataset limpio modificado\n",
    "out_dir = Path(p_mod).parent if 'p_mod' in globals() else Path('.')\n",
    "mod_clean_path = out_dir / 'cleaned_modified.csv'\n",
    "mod_clean.to_csv(mod_clean_path, index=False)\n",
    "\n",
    "print(\"Guardado:\", mod_clean_path)\n",
    "print(\"Tama√±o final ->\", mod_clean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SWH9ZJD7xzeF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWH9ZJD7xzeF",
    "outputId": "c522436e-30b4-40be-b411-2b141c9eb395"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mod = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_modified.csv\")\n",
    "print(\"Columnas disponibles:\")\n",
    "print(list(mod.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36f7a8",
   "metadata": {
    "id": "ed36f7a8"
   },
   "source": [
    "## 7) ¬øSe conserva la distribuci√≥n? Comparaci√≥n vs original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XUq4VjhAzPMA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUq4VjhAzPMA",
    "outputId": "c2b33d8c-32cd-467a-d732-706363592137"
   },
   "outputs": [],
   "source": [
    "#@title üß© Feature Engineering (funciona con o sin columna de fecha)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_path  = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_modified.csv\"\n",
    "out_path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched.csv\"\n",
    "\n",
    "mod = pd.read_csv(in_path)\n",
    "\n",
    "# Normaliza encabezados\n",
    "mod.columns = mod.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "print(\"Columnas detectadas:\", mod.columns.tolist())\n",
    "\n",
    "# === 1) Obtener/crear columna fecha ===\n",
    "fecha_col = None\n",
    "for c in mod.columns:\n",
    "    if \"date\" in c:  # por si viene 'date', 'datetime', 'dteday', etc.\n",
    "        fecha_col = c\n",
    "        break\n",
    "\n",
    "if fecha_col is not None:\n",
    "    # Parseo directo\n",
    "    dt = pd.to_datetime(mod[fecha_col], dayfirst=True, errors='coerce')\n",
    "else:\n",
    "    # Intento reconstruir con year/month/day si existen\n",
    "    if all(col in mod.columns for col in [\"year\", \"month\", \"day\"]):\n",
    "        dt = pd.to_datetime(\n",
    "            dict(year=mod[\"year\"], month=mod[\"month\"], day=mod[\"day\"]),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        fecha_col = \"fecha_reconstruida\"\n",
    "        mod[fecha_col] = dt\n",
    "        print(\"‚úÖ Fecha reconstruida desde year/month/day ‚Üí columna:\", fecha_col)\n",
    "    else:\n",
    "        dt = None\n",
    "        print(\"‚ö†Ô∏è No hay columna de fecha ni (year, month, day). Se omiten features de calendario.\")\n",
    "\n",
    "# === 2) Features temporales ===\n",
    "if dt is not None:\n",
    "    mod[\"dayofyear\"]  = dt.dt.dayofyear\n",
    "    # week can be UInt32 in pandas; convi√©rtela a int\n",
    "    mod[\"weekofyear\"] = dt.dt.isocalendar().week.astype(\"int64\")\n",
    "    mod[\"quarter\"]    = dt.dt.quarter\n",
    "    # Si no existe dayofweek/is_weekend los creamos\n",
    "    if \"dayofweek\" not in mod.columns:\n",
    "        mod[\"dayofweek\"] = dt.dt.dayofweek\n",
    "    if \"is_weekend\" not in mod.columns:\n",
    "        mod[\"is_weekend\"] = (mod[\"dayofweek\"] >= 5).astype(int)\n",
    "\n",
    "# === 3) Codificaci√≥n c√≠clica de la hora (si existe 'hour') ===\n",
    "if \"hour\" in mod.columns:\n",
    "    mod[\"hour_sin\"] = np.sin(2 * np.pi * mod[\"hour\"] / 24)\n",
    "    mod[\"hour_cos\"] = np.cos(2 * np.pi * mod[\"hour\"] / 24)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No existe columna 'hour'; se omiten hour_sin/hour_cos.\")\n",
    "\n",
    "# === 4) Interacciones y t√©rminos no lineales (si existen num√©ricas) ===\n",
    "if \"temperature(¬∞c)\" in mod.columns and \"humidity(%)\" in mod.columns:\n",
    "    mod[\"temp_humidity\"] = mod[\"temperature(¬∞c)\"] * mod[\"humidity(%)\"]\n",
    "if \"solar_radiation_(mj/m2)\" in mod.columns and \"wind_speed_(m/s)\" in mod.columns:\n",
    "    mod[\"solar_wind\"] = mod[\"solar_radiation_(mj/m2)\"] * mod[\"wind_speed_(m/s)\"]\n",
    "if \"temperature(¬∞c)\" in mod.columns:\n",
    "    mod[\"temp^2\"] = mod[\"temperature(¬∞c)\"] ** 2\n",
    "if \"humidity(%)\" in mod.columns:\n",
    "    mod[\"humidity^2\"] = mod[\"humidity(%)\"] ** 2\n",
    "\n",
    "# === 5) Guardar ===\n",
    "mod.to_csv(out_path, index=False)\n",
    "print(\"\\n‚úÖ Dataset enriquecido guardado.\")\n",
    "print(\"üìÇ\", out_path)\n",
    "print(\"üìè shape:\", mod.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-MSVcodzdGu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8-MSVcodzdGu",
    "outputId": "005f76d6-2125-472c-f57d-c42cf0a7a6ee"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ Reentrenamiento del modelo con el dataset enriquecido\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# === 1) Cargar el dataset enriquecido ===\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched.csv\"\n",
    "mod = pd.read_csv(path)\n",
    "\n",
    "print(\"‚úÖ Dataset cargado:\", mod.shape)\n",
    "print(\"Columnas disponibles:\", mod.columns.tolist())\n",
    "\n",
    "# === 2) Identificar target y variables ===\n",
    "target_col = \"rented_bike_count\"\n",
    "\n",
    "# Eliminar columnas no √∫tiles o duplicadas del target\n",
    "X = mod.drop(columns=[target_col], errors=\"ignore\")\n",
    "y = mod[target_col]\n",
    "\n",
    "# === 3) Codificar variables categ√≥ricas ===\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Categ√≥ricas detectadas:\", cat_cols.tolist())\n",
    "    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    X[cat_cols] = enc.fit_transform(X[cat_cols])\n",
    "else:\n",
    "    print(\"No se detectaron variables categ√≥ricas.\")\n",
    "\n",
    "# === 4) Definir modelo y validaci√≥n temporal ===\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "r2_scores, rmse_scores = [], []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "    r2_scores.append(r2)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "print(\"\\nüìà Resultados de validaci√≥n temporal:\")\n",
    "print(f\"R¬≤ promedio: {np.mean(r2_scores):.4f} ¬± {np.std(r2_scores):.4f}\")\n",
    "print(f\"RMSE promedio: {np.mean(rmse_scores):.2f}\")\n",
    "\n",
    "# === 5) Entrenar modelo final con todo el dataset ===\n",
    "model.fit(X, y)\n",
    "print(\"\\n‚úÖ Modelo final entrenado con todo el dataset.\")\n",
    "\n",
    "# === 6) Gr√°fico de residuales ===\n",
    "y_pred_all = model.predict(X)\n",
    "residuals = y_pred_all - y\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y, residuals, alpha=0.4)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Rented Bike Count (real)\")\n",
    "plt.ylabel(\"Residual (pred - real)\")\n",
    "plt.title(\"Residuals (modelo final)\")\n",
    "plt.show()\n",
    "\n",
    "# === 7) Importancia de variables ===\n",
    "importances = pd.DataFrame({\n",
    "    \"Variable\": X.columns,\n",
    "    \"Importancia\": model.feature_importances_\n",
    "}).sort_values(by=\"Importancia\", ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 15 variables m√°s importantes:\")\n",
    "display(importances.head(15))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(importances.head(15)[\"Variable\"], importances.head(15)[\"Importancia\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Importancia de las variables (Random Forest)\")\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y_Em4Y_U1veq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_Em4Y_U1veq",
    "outputId": "5b4076e1-745d-4318-d33c-7045c770d2b7"
   },
   "outputs": [],
   "source": [
    "#@title üß© Construir timestamp y crear *lags* y promedios m√≥viles (no-leak)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Carga el enriquecido\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# 2) Normaliza encabezados\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# 3) Reconstruye datetime si existe 'fecha_reconstruida' + 'hour'\n",
    "if \"fecha_reconstruida\" in df.columns and \"hour\" in df.columns:\n",
    "    dt = pd.to_datetime(df[\"fecha_reconstruida\"], errors=\"coerce\")\n",
    "    ts = dt + pd.to_timedelta(df[\"hour\"], unit=\"h\")\n",
    "    df[\"ts\"] = ts\n",
    "else:\n",
    "    # si no existe, intenta 'year','month','day','hour'\n",
    "    if all(c in df.columns for c in [\"year\",\"month\",\"day\",\"hour\"]):\n",
    "        dt = pd.to_datetime(dict(year=df[\"year\"], month=df[\"month\"], day=df[\"day\"]), errors=\"coerce\")\n",
    "        df[\"ts\"] = dt + pd.to_timedelta(df[\"hour\"], unit=\"h\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No pude construir un timestamp. Necesito fecha_reconstruida+hour o year+month+day+hour.\")\n",
    "\n",
    "# 4) Orden temporal\n",
    "df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "# 5) Crear lags SIN fuga (shift desplaza hacia atr√°s)\n",
    "y = df[\"rented_bike_count\"]\n",
    "df[\"lag_1h\"]   = y.shift(1)\n",
    "df[\"lag_24h\"]  = y.shift(24)       # mismo d√≠a anterior, misma hora\n",
    "df[\"lag_168h\"] = y.shift(24*7)     # una semana antes, misma hora\n",
    "\n",
    "# 6) Promedios m√≥viles usando solo pasado (shift(1) evita mirar el futuro)\n",
    "df[\"roll_mean_24h\"]  = y.shift(1).rolling(24).mean()\n",
    "df[\"roll_mean_168h\"] = y.shift(1).rolling(24*7).mean()\n",
    "df[\"roll_max_24h\"]   = y.shift(1).rolling(24).max()\n",
    "df[\"roll_min_24h\"]   = y.shift(1).rolling(24).min()\n",
    "\n",
    "# 7) Elimina las filas al inicio que quedan con NaN por lags\n",
    "min_warmup = 24*7   # 1 semana\n",
    "df = df.iloc[min_warmup:].copy()\n",
    "\n",
    "# 8) Guardar versi√≥n con lags\n",
    "out_path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"‚úÖ Dataset con lags guardado:\", out_path)\n",
    "print(\"shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brw2ekA310FF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "brw2ekA310FF",
    "outputId": "f7c7bd32-3cf8-4039-8358-e414b25bbc7b"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ Entrenar con TimeSeriesSplit usando lags (evita usar la fecha cruda)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# 1) Carga\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "target = \"rented_bike_count\"\n",
    "\n",
    "# 2) ARMAR X (sin usar la fecha cruda). Dejamos 'ts' solo para ordenar, NO como feature.\n",
    "drop_cols = [target, \"ts\", \"fecha_reconstruida\"]  # sacamos la fecha categ√≥rica\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[target]\n",
    "\n",
    "# 3) Detectar tipos\n",
    "cat_cols = X.select_dtypes(include=[\"object\",\"bool\"]).columns.tolist()\n",
    "num_cols = X.columns.difference(cat_cols).tolist()\n",
    "\n",
    "print(\"Categ√≥ricas:\", cat_cols)\n",
    "print(\"Num√©ricas (ejemplo):\", num_cols[:10], \"...\")\n",
    "\n",
    "# 4) Preprocesamiento\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 5) Modelo principal (HGBR suele ir mejor que RF con muchas num√©ricas)\n",
    "# Si HGBR te da problemas en tu versi√≥n, cambia 'reg' por RandomForestRegressor(...)\n",
    "reg = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.08,\n",
    "    max_depth=10,\n",
    "    max_iter=400,\n",
    "    l2_regularization=0.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"prep\", prep), (\"reg\", reg)])\n",
    "\n",
    "# 6) Validaci√≥n temporal\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "r2s, rmses = [], []\n",
    "\n",
    "for fold, (tr, te) in enumerate(tscv.split(X), 1):\n",
    "    Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "    ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    pred = pipe.predict(Xte)\n",
    "\n",
    "    r2 = r2_score(yte, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(yte, pred))\n",
    "    r2s.append(r2); rmses.append(rmse)\n",
    "    print(f\"Fold {fold}: R¬≤={r2:.4f}  RMSE={rmse:.2f}\")\n",
    "\n",
    "print(\"\\nüìà CV temporal ‚Üí R¬≤ mean:\", round(np.mean(r2s),4), \"¬±\", round(np.std(r2s),4))\n",
    "print(\"üìâ CV temporal ‚Üí RMSE mean:\", round(np.mean(rmses),2))\n",
    "\n",
    "# 7) Fit final y residuales (80/20 temporal)\n",
    "split_idx = int(len(X)*0.8)\n",
    "pipe.fit(X.iloc[:split_idx], y.iloc[:split_idx])\n",
    "y_pred = pipe.predict(X.iloc[split_idx:])\n",
    "\n",
    "r2 = r2_score(y.iloc[split_idx:], y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y.iloc[split_idx:], y_pred))\n",
    "print(f\"\\n‚úÖ Test temporal 80/20: R¬≤={r2:.4f}  RMSE={rmse:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y.iloc[split_idx:], y_pred - y.iloc[split_idx:], alpha=0.4)\n",
    "plt.axhline(0, color=\"red\", ls=\"--\")\n",
    "plt.xlabel(\"Rented Bike Count (real)\")\n",
    "plt.ylabel(\"Residual (pred - real)\")\n",
    "plt.title(\"Residuales (con lags)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KPT8WJas2oCs",
   "metadata": {
    "id": "KPT8WJas2oCs"
   },
   "outputs": [],
   "source": [
    "#@title üîß GridSearch para optimizar el modelo (HGBR o RF)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "\n",
    "# Cargar dataset enriquecido con lags\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "target = \"rented_bike_count\"\n",
    "drop_cols = [target, \"ts\", \"fecha_reconstruida\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[target]\n",
    "\n",
    "# Identificar tipos\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "num_cols = X.columns.difference(cat_cols).tolist()\n",
    "\n",
    "# Preprocesador\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHuRsC8s2rfU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nHuRsC8s2rfU",
    "outputId": "5a2370aa-fe46-40fb-aec6-3908395559cd"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ Configurar b√∫squeda de hiperpar√°metros\n",
    "reg = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "pipe = Pipeline(steps=[(\"prep\", prep), (\"reg\", reg)])\n",
    "\n",
    "param_grid = {\n",
    "    \"reg__learning_rate\": [0.03, 0.05, 0.08, 0.1],\n",
    "    \"reg__max_depth\": [8, 10, 12, None],\n",
    "    \"reg__max_iter\": [300, 400, 500],\n",
    "    \"reg__l2_regularization\": [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=make_scorer(r2_score),\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhtE2dIa55H3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhtE2dIa55H3",
    "outputId": "ebe919ea-9a6b-4d78-cf16-6e0c32b9d6f9"
   },
   "outputs": [],
   "source": [
    "#@title üìà Resultados del mejor modelo\n",
    "print(\"üèÜ Mejor combinaci√≥n encontrada:\")\n",
    "print(grid.best_params_)\n",
    "print(\"\\nüîπ Mejor R¬≤ promedio en CV temporal:\", round(grid.best_score_, 4))\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Entrena final con todo el dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Guardar modelo (opcional)\n",
    "import joblib\n",
    "joblib.dump(best_model, \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/best_model.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ Modelo optimizado guardado como best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tUoyy_hH70R7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "tUoyy_hH70R7",
    "outputId": "1bd25b43-133b-4403-a42f-854fc9f9f111"
   },
   "outputs": [],
   "source": [
    "# üìä Top combinaciones del GridSearch\n",
    "cv = pd.DataFrame(grid.cv_results_)\n",
    "cols = [\"mean_test_score\",\"std_test_score\",\"rank_test_score\"] + [c for c in cv.columns if c.startswith(\"param_\")]\n",
    "top = cv.sort_values(\"rank_test_score\").head(15)[cols]\n",
    "top.rename(columns={\"mean_test_score\":\"R2_CV_mean\",\"std_test_score\":\"R2_CV_std\",\"rank_test_score\":\"rank\"}, inplace=True)\n",
    "display(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rR6DqfFD75Le",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "rR6DqfFD75Le",
    "outputId": "1de635ca-7cd9-43eb-838f-6d82235dae49"
   },
   "outputs": [],
   "source": [
    "#@title üèÅ Evaluaci√≥n final 80/20 temporal + export de predicciones\n",
    "import pandas as pd, numpy as np, joblib, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1) Carga dataset con lags (ordenado por ts) y rearmar X,y como en el grid\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "target = \"rented_bike_count\"\n",
    "drop_cols = [target, \"ts\", \"fecha_reconstruida\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[target]\n",
    "\n",
    "# 2) Cargar el mejor modelo ya entrenado por GridSearch\n",
    "best_model = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/best_model.pkl\")\n",
    "\n",
    "# 3) Split temporal 80/20, fit y m√©tricas\n",
    "split_idx = int(len(X)*0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"‚úÖ Test temporal 80/20 ‚Äî R¬≤: {r2:.4f}   RMSE: {rmse:.2f}\")\n",
    "\n",
    "# 4) Residuales\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_test, y_pred - y_test, alpha=0.4)\n",
    "plt.axhline(0, color=\"red\", ls=\"--\")\n",
    "plt.xlabel(\"Rented Bike Count (real)\")\n",
    "plt.ylabel(\"Residual (pred - real)\")\n",
    "plt.title(\"Residuales del mejor modelo (80/20 temporal)\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Export de predicciones\n",
    "preds = pd.DataFrame({\n",
    "    \"ts\": df.loc[split_idx:, \"ts\"].values if \"ts\" in df.columns else np.arange(len(y_test)),\n",
    "    \"y_real\": y_test.values,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"residual\": y_pred - y_test.values\n",
    "})\n",
    "out_csv = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/test_predictions_best_model.csv\"\n",
    "preds.to_csv(out_csv, index=False)\n",
    "print(\"üíæ Predicciones guardadas en:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V1RNtqS38IUr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1RNtqS38IUr",
    "outputId": "6e6462a7-480a-45b2-a82d-3072d65648b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_fino = {\n",
    "    \"reg__learning_rate\": [0.07, 0.09, 0.1, 0.11],\n",
    "    \"reg__max_depth\": [10, 12, 14],\n",
    "    \"reg__max_iter\": [400, 500, 600],\n",
    "    \"reg__l2_regularization\": [0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "grid_fino = GridSearchCV(\n",
    "    grid.best_estimator_,\n",
    "    param_grid=param_grid_fino,\n",
    "    cv=tscv,\n",
    "    scoring=make_scorer(r2_score),\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_fino.fit(X, y)\n",
    "print(\"Nuevo mejor R¬≤ promedio:\", round(grid_fino.best_score_, 4))\n",
    "print(\"Mejores par√°metros:\", grid_fino.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gr-R3RpQ_IsK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gr-R3RpQ_IsK",
    "outputId": "925c6a18-a6bc-48b9-d1e2-c38765aca930"
   },
   "outputs": [],
   "source": [
    "#@title üß© Features suaves: rolling/lag/interaction (sin fuga) + guardado\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_path  = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags.csv\"\n",
    "out_path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags_plus.csv\"\n",
    "\n",
    "df = pd.read_csv(in_path)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Asegura orden temporal por 'ts' si existe\n",
    "if \"ts\" in df.columns:\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "else:\n",
    "    # Si no hay ts, intenta reconstruir para mantener orden\n",
    "    if all(c in df.columns for c in [\"year\",\"month\",\"day\",\"hour\"]):\n",
    "        base_dt = pd.to_datetime(dict(year=df[\"year\"], month=df[\"month\"], day=df[\"day\"]), errors=\"coerce\")\n",
    "        df[\"ts\"] = base_dt + pd.to_timedelta(df[\"hour\"], unit=\"h\")\n",
    "        df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "# --- 1) Rolling de 6h en temperatura (shift(1) evita ver el futuro)\n",
    "if \"temperature(¬∞c)\" in df.columns:\n",
    "    df[\"temp_roll_6h\"] = df[\"temperature(¬∞c)\"].shift(1).rolling(6).mean()\n",
    "\n",
    "# --- 2) Rolling de 3h en lluvia\n",
    "if \"rainfall(mm)\" in df.columns:\n",
    "    df[\"rain_roll_3h\"] = df[\"rainfall(mm)\"].shift(1).rolling(3).mean()\n",
    "\n",
    "# --- 3) Lag 24h del target (si no existiera o quieres actualizarlo)\n",
    "if \"rented_bike_count\" in df.columns:\n",
    "    df[\"lag_24h\"] = df[\"rented_bike_count\"].shift(24)\n",
    "\n",
    "# --- 4) Interacci√≥n temperatura x radiaci√≥n solar\n",
    "if set([\"temperature(¬∞c)\", \"solar_radiation_(mj/m2)\"]).issubset(df.columns):\n",
    "    df[\"temp_x_solar\"] = df[\"temperature(¬∞c)\"] * df[\"solar_radiation_(mj/m2)\"]\n",
    "\n",
    "# Warmup: elimina filas con NaN por lags/rollings (tomamos 24h)\n",
    "warmup = 24\n",
    "df = df.iloc[warmup:].copy()\n",
    "\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"‚úÖ Guardado con nuevas features en:\", out_path)\n",
    "print(\"üìè shape:\", df.shape)\n",
    "print(\"üÜï Nuevas columnas:\", [c for c in df.columns if c in [\"temp_roll_6h\",\"rain_roll_3h\",\"lag_24h\",\"temp_x_solar\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6y8DECk_Lkv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i6y8DECk_Lkv",
    "outputId": "db2886bb-bb78-4ce0-ed19-8f8b2004d26c"
   },
   "outputs": [],
   "source": [
    "#@title üìä Reporte visual: m√©tricas y gr√°ficos (80/20 temporal)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE/cleaned_enriched_lags_plus.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Orden temporal\n",
    "if \"ts\" in df.columns:\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "    df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "target = \"rented_bike_count\"\n",
    "drop_cols = [target, \"ts\", \"fecha_reconstruida\"]  # no usamos la fecha cruda como feature\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "y = df[target]\n",
    "\n",
    "# Categ√≥ricas / num√©ricas\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "num_cols = X.columns.difference(cat_cols).tolist()\n",
    "\n",
    "prep = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Modelo (ajusta si quieres los params encontrados en GridSearch)\n",
    "reg = HistGradientBoostingRegressor(\n",
    "    learning_rate=0.1, max_depth=12, max_iter=500,\n",
    "    l2_regularization=0.5, random_state=42\n",
    ")\n",
    "pipe = Pipeline(steps=[(\"prep\", prep), (\"reg\", reg)])\n",
    "\n",
    "# Split 80/20 temporal\n",
    "split_idx = int(len(X)*0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae  = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"‚úÖ Test temporal 80/20 ‚Üí R¬≤: {r2:.4f} | RMSE: {rmse:.2f} | MAE: {mae:.2f}\")\n",
    "\n",
    "# === 1) Importancia de variables (robusta) ===\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "try:\n",
    "    # Preferido: PI sobre las columnas ORIGINALES (mismas longitudes)\n",
    "    pi = permutation_importance(\n",
    "        pipe, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    feat_names = X_test.columns.tolist()                      # <-- clave\n",
    "    imp_df = pd.DataFrame(\n",
    "        {\"Variable\": feat_names, \"Importance\": pi.importances_mean}\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback: usar feature_importances_ del modelo en el ESPACIO TRANSFORMADO\n",
    "    reg_est = pipe.named_steps[\"reg\"]\n",
    "    if not hasattr(reg_est, \"feature_importances_\"):\n",
    "        raise RuntimeError(\n",
    "            \"No se pudo calcular importancias: el estimador no expone 'feature_importances_'.\"\n",
    "        )\n",
    "\n",
    "    # Nombres de columnas despu√©s del preprocesamiento\n",
    "    try:\n",
    "        feat_names = pipe.named_steps[\"prep\"].get_feature_names_out().tolist()\n",
    "    except Exception:\n",
    "        oh = None\n",
    "        if cat_cols:\n",
    "            oh = pipe.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps.get(\"oh\", None)\n",
    "        oh_names = oh.get_feature_names_out(cat_cols).tolist() if oh is not None else []\n",
    "        feat_names = list(num_cols) + oh_names\n",
    "\n",
    "    imp_df = pd.DataFrame(\n",
    "        {\"Variable\": feat_names, \"Importance\": reg_est.feature_importances_}\n",
    "    )\n",
    "\n",
    "# Ordenar y mostrar\n",
    "imp_df = imp_df.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 15 variables m√°s importantes:\")\n",
    "display(imp_df.head(15))\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.barh(imp_df.head(15)[\"Variable\"][::-1], imp_df.head(15)[\"Importance\"][::-1])\n",
    "plt.title(\"Importancia de variables\")\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae915e11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "ae915e11",
    "outputId": "29a4ab13-3cc7-4bc9-93fc-eff44f77decb"
   },
   "outputs": [],
   "source": [
    "#@title üß™ Stats lado a lado + KS test (robusto: detecta target y reconstruye mod_clean si falta)\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# ------- Utilidades robustas -------\n",
    "def normalize_cols_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # min√∫sculas, sin espacios dobles, reemplaza espacios por _\n",
    "    df.columns = (df.columns\n",
    "                  .str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                  .str.replace(\" \", \"_\"))\n",
    "    return df\n",
    "\n",
    "def detect_target_column(cols) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el nombre 'real' del target entre las columnas,\n",
    "    buscando variantes comunes de 'Rented Bike Count'.\n",
    "    \"\"\"\n",
    "    candidates_exact = [\n",
    "        \"rented_bike_count\",\n",
    "        \"rentedbikecount\",\n",
    "        \"rented_bikes_count\"\n",
    "    ]\n",
    "    # primero intenta coincidencia exacta (normalizada)\n",
    "    for c in candidates_exact:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    # luego intenta heur√≠stica por tokens\n",
    "    pat = re.compile(r\"(rented).*?(bike).*?(count)\", re.I)\n",
    "    for c in cols:\n",
    "        if pat.search(c.replace(\"_\", \" \")):\n",
    "            return c\n",
    "    # si no encuentra, intenta coincidencia parcial\n",
    "    for c in cols:\n",
    "        if \"rented\" in c and \"bike\" in c and \"count\" in c:\n",
    "            return c\n",
    "    raise KeyError(\"No pude detectar la columna target (rented_bike_count). Revisa nombres de columnas.\")\n",
    "\n",
    "# ------- 1) Asegurar org_clean -------\n",
    "if 'p_org' not in globals():\n",
    "    raise RuntimeError(\"No encuentro 'p_org'. Ejecuta la celda de montaje/carga de rutas (la que define p_org).\")\n",
    "\n",
    "org = pd.read_csv(p_org)\n",
    "org = normalize_cols_df(org)\n",
    "\n",
    "# Si tienes funciones propias de limpieza, apl√≠calas (opcionales)\n",
    "if 'clean_df' in globals():\n",
    "    # Primero detecta target en org para que clean_df pueda usarlo si lo requiere\n",
    "    try:\n",
    "        target_col_org = detect_target_column(list(org.columns))\n",
    "    except KeyError:\n",
    "        target_col_org = None\n",
    "    org_clean = clean_df(org, target_col_org) if target_col_org else clean_df(org, None)\n",
    "    org_clean = normalize_cols_df(org_clean)\n",
    "else:\n",
    "    # fallback simple si no existe clean_df\n",
    "    org_clean = org.copy()\n",
    "    # elimina columnas completamente vac√≠as\n",
    "    org_clean = org_clean.dropna(axis=1, how='all')\n",
    "\n",
    "# Detecta target definitivo en org_clean\n",
    "target_col = detect_target_column(list(org_clean.columns))\n",
    "\n",
    "# convertir target a num√©rico por consistencia\n",
    "org_clean[target_col] = pd.to_numeric(org_clean[target_col], errors=\"coerce\")\n",
    "\n",
    "# ------- 2) Asegurar mod_clean -------\n",
    "if 'mod_clean' not in globals():\n",
    "    # intenta reconstruir desde tu CSV limpio\n",
    "    from pathlib import Path\n",
    "    base_dir = \"/content/drive/MyDrive/Colab Notebooks/SEXTO TRIMESTRE\"\n",
    "    # prioriza el archivo m√°s completo si existe\n",
    "    candidates = [\n",
    "        \"cleaned_modified.csv\",\n",
    "        \"cleaned_enriched.csv\",\n",
    "        \"cleaned_enriched_lags.csv\",\n",
    "        \"cleaned_enriched_lags_plus.csv\"\n",
    "    ]\n",
    "    chosen = None\n",
    "    for name in candidates:\n",
    "        p = Path(base_dir) / name\n",
    "        if p.exists():\n",
    "            chosen = str(p)\n",
    "            break\n",
    "    if chosen is None:\n",
    "        raise RuntimeError(\"No encuentro 'mod_clean' ni archivos limpios en Drive (cleaned_modified.csv / enriched...).\")\n",
    "    mod_clean = pd.read_csv(chosen)\n",
    "    mod_clean = normalize_cols_df(mod_clean)\n",
    "else:\n",
    "    mod_clean = normalize_cols_df(mod_clean)\n",
    "\n",
    "# Alinea el nombre del target tambi√©n en mod_clean (por si difiere)\n",
    "try:\n",
    "    target_col_mod = detect_target_column(list(mod_clean.columns))\n",
    "except KeyError:\n",
    "    target_col_mod = target_col  # usa el mismo nombre si coincide tras normalizar\n",
    "\n",
    "# asegurar tipo num√©rico en el target de mod\n",
    "if target_col_mod in mod_clean.columns:\n",
    "    mod_clean[target_col_mod] = pd.to_numeric(mod_clean[target_col_mod], errors=\"coerce\")\n",
    "\n",
    "# ------- 3) KS test en columnas comunes num√©ricas -------\n",
    "common = [c for c in mod_clean.columns if c in org_clean.columns]\n",
    "\n",
    "rows = []\n",
    "for c in common:\n",
    "    # saltar columnas claramente no comparables (ids, timestamps, texto duro)\n",
    "    if c in {target_col, target_col_mod, \"ts\", \"fecha_reconstruida\"}:\n",
    "        pass  # puedes comentar esta l√≠nea si quieres comparar tambi√©n el target\n",
    "    a = pd.to_numeric(mod_clean[c], errors=\"coerce\").dropna()\n",
    "    b = pd.to_numeric(org_clean[c], errors=\"coerce\").dropna()\n",
    "    if len(a) > 50 and len(b) > 50:\n",
    "        stat, p = ks_2samp(a, b)\n",
    "        rows.append({\n",
    "            \"col\": c,\n",
    "            \"mean_mod\": a.mean(),\n",
    "            \"std_mod\": a.std(),\n",
    "            \"p50_mod\": a.median(),\n",
    "            \"mean_org\": b.mean(),\n",
    "            \"std_org\": b.std(),\n",
    "            \"p50_org\": b.median(),\n",
    "            \"ks_stat\": stat,\n",
    "            \"ks_pvalue\": p\n",
    "        })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows).sort_values(\"ks_stat\", ascending=False)\n",
    "\n",
    "if len(cmp_df) > 0:\n",
    "    display(cmp_df.style.set_caption(\n",
    "        \"Comparaci√≥n (modified limpio vs. original limpio) ‚Äî KS p>0.05 ‚âà sin cambio estad√≠stico fuerte\"\n",
    "    ))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hubo suficientes columnas num√©ricas comunes con datos en ambos datasets para comparar con KS.\")\n",
    "print(f\"Target usado para org_clean: '{target_col}'  |  para mod_clean: '{target_col_mod}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ebe56",
   "metadata": {
    "id": "735ebe56"
   },
   "source": [
    "## 8) Baseline ML (solo modified limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ERWKvUk7TSke",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "ERWKvUk7TSke",
    "outputId": "0d14db50-9dde-40fb-ee74-e24672011199"
   },
   "outputs": [],
   "source": [
    "#@title 8 BIS) Baseline ML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ------------------ Par√°metros editables ------------------\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "RANDOM_STATE = 42\n",
    "DO_STANDARDIZE = False          # √°rboles no lo necesitan\n",
    "N_ESTIMATORS = 500              # un poco m√°s robusto\n",
    "MAX_DEPTH = 20                  # profundidad moderada\n",
    "MIN_SAMPLES_LEAF = 2            # hoja m√≠nima para reducir overfitting\n",
    "MAX_FEATURES = 'sqrt'           # estrategia com√∫n en RF\n",
    "N_PERMUT = 10\n",
    "TOPK_IMP = 15\n",
    "USE_SEASONS_AS_ORDINAL = False  # Seasons como nominal por ciclo anual\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# --- Chequeos previos ---\n",
    "if 'mod_clean' not in globals():\n",
    "    raise RuntimeError(\"No se encontr√≥ 'mod_clean'. Ejecuta la celda de limpieza primero.\")\n",
    "if 'target_col' not in globals() or target_col not in mod_clean.columns:\n",
    "    raise KeyError(\"No se encontr√≥ 'target_col' o no est√° en mod_clean.columns\")\n",
    "\n",
    "# Copia de trabajo + salvaguarda por si mixed sigue ah√≠\n",
    "df = mod_clean.copy()\n",
    "if \"mixed_type_col\" in df.columns:\n",
    "    df = df.drop(columns=[\"mixed_type_col\"])\n",
    "\n",
    "# Target num√©rico + drop filas sin target\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "# ---------- Feature engineering c√≠clico para Hour ----------\n",
    "# Si existe Hour (o __Hour), convierte a num y crea sin/cos\n",
    "hour_candidates = [c for c in ['Hour', '__Hour'] if c in df.columns]\n",
    "if hour_candidates:\n",
    "    hcol = hour_candidates[0]\n",
    "    df[hcol] = pd.to_numeric(df[hcol], errors='coerce')\n",
    "    # reemplazar outliers/NaN razonablemente\n",
    "    df[hcol] = df[hcol].clip(0, 23)\n",
    "    df['Hour_sin'] = np.sin(2*np.pi*df[hcol]/24.0)\n",
    "    df['Hour_cos'] = np.cos(2*np.pi*df[hcol]/24.0)\n",
    "    # (Opcional) si quieres evitar doble conteo, puedes eliminar la original:\n",
    "    # df.drop(columns=[hcol], inplace=True)\n",
    "\n",
    "y = df[target_col].astype(float)\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Detectar tipos base\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_all  = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# --- Definir ordinales vs nominales ---\n",
    "ordinal_specs_all = []\n",
    "\n",
    "# Seasons -> por defecto NOMINAL (USE_SEASONS_AS_ORDINAL=False)\n",
    "if 'Seasons' in X.columns and USE_SEASONS_AS_ORDINAL:\n",
    "    ordinal_specs_all.append((\"Seasons\", [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]))\n",
    "\n",
    "# Functioning Day: binaria ordenada (si existe)\n",
    "if 'Functioning Day' in X.columns:\n",
    "    ordinal_specs_all.append((\"Functioning Day\", [\"No\", \"Yes\"]))\n",
    "\n",
    "ordinal_cols   = [name for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "ordinal_orders = [order for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "cat_nominal = [c for c in cat_all if c not in ordinal_cols]\n",
    "\n",
    "print(f\"Dataset final: {df.shape} | Num√©ricas: {len(num_cols)} | \"\n",
    "      f\"Cat nominales: {len(cat_nominal)} | Cat ordinales: {len(ordinal_cols)} | Target n: {len(y)}\")\n",
    "\n",
    "# ---------- Estratificaci√≥n (binning si regresi√≥n) ----------\n",
    "is_classification = False\n",
    "if y.nunique() <= 20:\n",
    "    is_classification = True\n",
    "\n",
    "if is_classification:\n",
    "    stratify_labels = y\n",
    "else:\n",
    "    n_bins = min(10, int(np.sqrt(len(y))))\n",
    "    try:\n",
    "        stratify_labels = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "    except Exception:\n",
    "        stratify_labels = pd.cut(y, bins=n_bins)\n",
    "\n",
    "# ---------- Split: test y luego val ----------\n",
    "X_rest, X_test, y_rest, y_test, strat_rest, strat_test = train_test_split(\n",
    "    X, y, stratify_labels, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "val_frac_of_rest = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_rest, y_rest, test_size=val_frac_of_rest, random_state=RANDOM_STATE, stratify=strat_rest\n",
    ")\n",
    "\n",
    "print(f\"Split sizes -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# ---------- Preprocesamiento ----------\n",
    "num_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler() if DO_STANDARDIZE else \"passthrough\")\n",
    "])\n",
    "\n",
    "# OneHot compatible con distintas versiones de sklearn\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn < 1.2\n",
    "\n",
    "cat_nominal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "cat_ordinal_transformer = None\n",
    "if ordinal_cols:\n",
    "    cat_ordinal_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ordinal\", OrdinalEncoder(\n",
    "            categories=ordinal_orders,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_transformer, num_cols))\n",
    "if cat_nominal:\n",
    "    transformers.append((\"cat_nom\", cat_nominal_transformer, cat_nominal))\n",
    "if ordinal_cols and cat_ordinal_transformer is not None:\n",
    "    transformers.append((\"cat_ord\", cat_ordinal_transformer, ordinal_cols))\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# ---------- Modelo ----------\n",
    "if is_classification:\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE,\n",
    "        min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES, n_jobs=-1\n",
    "    )\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", model)])\n",
    "\n",
    "# Entrenar\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# ---------- M√©tricas ----------\n",
    "if is_classification:\n",
    "    print(\"\\nVALIDACI√ìN - Clasificaci√≥n:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"TEST - Clasificaci√≥n:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "else:\n",
    "    print(\"\\nVALIDACI√ìN - Regresi√≥n:\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_val, y_val_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_val, y_val_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "    print(\"R2:\", r2_score(y_val, y_val_pred))\n",
    "    print(\"\\nTEST - Regresi√≥n (evaluaci√≥n final):\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "    print(\"R2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "# ---------- Importancia por permutaci√≥n (alineada a columnas de ENTRADA) ----------\n",
    "try:\n",
    "    input_feature_names = []\n",
    "    if num_cols:      input_feature_names += list(num_cols)\n",
    "    if cat_nominal:   input_feature_names += list(cat_nominal)\n",
    "    if ordinal_cols:  input_feature_names += list(ordinal_cols)\n",
    "\n",
    "    result = permutation_importance(\n",
    "        pipe, X_val, y_val, n_repeats=N_PERMUT, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    imp = pd.Series(result.importances_mean, index=input_feature_names).sort_values(ascending=False)\n",
    "    print(f\"\\nTop {min(TOPK_IMP, len(imp))} features por permutaci√≥n (nivel columnas de entrada):\")\n",
    "    display(imp.head(TOPK_IMP))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo calcular la importancia por permutaci√≥n:\", e)\n",
    "\n",
    "print(\"\\nNotas:\")\n",
    "print(\"- Seasons se trata como NOMINAL por defecto (switch USE_SEASONS_AS_ORDINAL=False).\")\n",
    "print(\"- Hour ahora incluye Hour_sin/Hour_cos; se puede eliminar la columna Hour original si se dejo.\")\n",
    "print(\"- √Årboles sin escalado; RF con min_samples_leaf y max_features para mejor generalizaci√≥n.\")\n",
    "print(\"- Guarda el pipeline con joblib.dump(pipe, 'best_model.joblib').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ckbpqHeuVAcV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ckbpqHeuVAcV",
    "outputId": "77e4df23-0dc1-4d0c-b97b-413faee2ab52"
   },
   "outputs": [],
   "source": [
    "#@title 9) XGBoost + features temporales/derivadas + LOG1P + MAPE + permutaci√≥n\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ------------------ Par√°metros editables ------------------\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Preprocesamiento\n",
    "DO_STANDARDIZE_NUM = False      # boosting no lo necesita\n",
    "USE_SEASONS_AS_ORDINAL = False  # Seasons como nominal (ciclo anual)\n",
    "USE_HOUR_SIN_COS = True         # mantener Hour_sin / Hour_cos si se generaron antes\n",
    "\n",
    "# Modelo (ajusta aqu√≠)\n",
    "TRY_XGBOOST = True              # si no est√° instalado, fallback a HistGradientBoosting\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "HGB_PARAMS = dict(\n",
    "    max_depth=None, learning_rate=0.05, max_iter=600, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Target\n",
    "LOG1P_TARGET = True            # activar para estabilizar varianza del target\n",
    "N_PERMUT = 10\n",
    "TOPK_IMP = 15\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# --- Chequeos y base ---\n",
    "if 'mod_clean' not in globals():\n",
    "    raise RuntimeError(\"No se encontr√≥ 'mod_clean'. Ejecuta la celda de limpieza primero.\")\n",
    "if 'target_col' not in globals() or target_col not in mod_clean.columns:\n",
    "    raise KeyError(\"No se encontr√≥ 'target_col' o no est√° en mod_clean.columns\")\n",
    "\n",
    "df = mod_clean.copy()\n",
    "if \"mixed_type_col\" in df.columns:\n",
    "    df = df.drop(columns=[\"mixed_type_col\"])\n",
    "\n",
    "# --- Features temporales desde Date ---\n",
    "def ensure_datetime(s):\n",
    "    return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    dt = ensure_datetime(df['Date'])\n",
    "    df['Year']    = dt.dt.year\n",
    "    df['Month']   = dt.dt.month\n",
    "    df['Weekday'] = dt.dt.dayofweek  # 0=Lunes ... 6=Domingo\n",
    "    df['is_weekend'] = (df['Weekday'] >= 5).astype(int)\n",
    "    # Codificaci√≥n c√≠clica Month/Weekday\n",
    "    df['Month_sin']   = np.sin(2*np.pi*(df['Month']-1)/12.0)\n",
    "    df['Month_cos']   = np.cos(2*np.pi*(df['Month']-1)/12.0)\n",
    "    df['Weekday_sin'] = np.sin(2*np.pi*df['Weekday']/7.0)\n",
    "    df['Weekday_cos'] = np.cos(2*np.pi*df['Weekday']/7.0)\n",
    "\n",
    "# Si existen Hour/__Hour y no se crearon sin/cos antes, activarlo aqu√≠\n",
    "if USE_HOUR_SIN_COS:\n",
    "    for hcol in ['Hour', '__Hour']:\n",
    "        if hcol in df.columns:\n",
    "            df[hcol] = pd.to_numeric(df[hcol], errors='coerce').clip(0, 23)\n",
    "            if 'Hour_sin' not in df.columns:\n",
    "                df['Hour_sin'] = np.sin(2*np.pi*df[hcol]/24.0)\n",
    "            if 'Hour_cos' not in df.columns:\n",
    "                df['Hour_cos'] = np.cos(2*np.pi*df[hcol]/24.0)\n",
    "            # (Opcional) evita duplicidad si dejas sin/cos\n",
    "            # df.drop(columns=[hcol], inplace=True)\n",
    "            break\n",
    "\n",
    "# --- Features DERIVADAS √∫tiles ---\n",
    "# Hora pico\n",
    "if 'Hour' in df.columns:\n",
    "    df['is_rush_hour'] = df['Hour'].isin([7,8,9,17,18,19]).astype(int)\n",
    "elif '__Hour' in df.columns:\n",
    "    df['is_rush_hour'] = df['__Hour'].isin([7,8,9,17,18,19]).astype(int)\n",
    "\n",
    "# Comodidad percibida aprox.\n",
    "if {'Temperature(¬∞C)', 'Humidity(%)'}.issubset(df.columns):\n",
    "    df['comfort_index'] = df['Temperature(¬∞C)'] - df['Humidity(%)']/5.0\n",
    "\n",
    "# Disconfort por viento\n",
    "if {'Wind speed (m/s)', 'Humidity(%)'}.issubset(df.columns):\n",
    "    df['wind_discomfort'] = df['Wind speed (m/s)'] * df['Humidity(%)']\n",
    "\n",
    "# Feriado o fin de semana (normaliza Holiday a binaria)\n",
    "if 'is_weekend' in df.columns and 'Holiday' in df.columns:\n",
    "    h = df['Holiday'].astype(str).str.lower().isin(['holiday','yes','1','true'])\n",
    "    df['is_holiday_or_weekend'] = ((df['is_weekend'] == 1) | h).astype(int)\n",
    "\n",
    "# --- Target y split base ---\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "if LOG1P_TARGET:\n",
    "    y_raw = df[target_col].astype(float)\n",
    "    y = np.log1p(y_raw)\n",
    "else:\n",
    "    y = df[target_col].astype(float)\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Detectar tipos\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_all  = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# Definir ordinales vs nominales\n",
    "ordinal_specs_all = []\n",
    "if 'Seasons' in X.columns and USE_SEASONS_AS_ORDINAL:\n",
    "    ordinal_specs_all.append((\"Seasons\", [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]))\n",
    "if 'Functioning Day' in X.columns:\n",
    "    ordinal_specs_all.append((\"Functioning Day\", [\"No\", \"Yes\"]))\n",
    "\n",
    "ordinal_cols   = [name for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "ordinal_orders = [order for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "cat_nominal = [c for c in cat_all if c not in ordinal_cols]\n",
    "\n",
    "print(f\"Dataset final: {df.shape} | Num√©ricas: {len(num_cols)} | \"\n",
    "      f\"Cat nominales: {len(cat_nominal)} | Cat ordinales: {len(ordinal_cols)} | Target n: {len(y)}\")\n",
    "\n",
    "# Estratificaci√≥n por bins (regresi√≥n)\n",
    "n_bins = min(10, int(np.sqrt(len(y))))\n",
    "try:\n",
    "    stratify_labels = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "except Exception:\n",
    "    stratify_labels = pd.cut(y, bins=n_bins)\n",
    "\n",
    "X_rest, X_test, y_rest, y_test, strat_rest, strat_test = train_test_split(\n",
    "    X, y, stratify_labels, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "val_frac_of_rest = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_rest, y_rest, test_size=val_frac_of_rest, random_state=RANDOM_STATE, stratify=strat_rest\n",
    ")\n",
    "print(f\"Split sizes -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "if DO_STANDARDIZE_NUM:\n",
    "    num_steps.append((\"scaler\", StandardScaler()))\n",
    "num_transformer = Pipeline(steps=num_steps)\n",
    "\n",
    "# OneHot compatible versiones\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn < 1.2\n",
    "\n",
    "cat_nominal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "cat_ordinal_transformer = None\n",
    "if ordinal_cols:\n",
    "    cat_ordinal_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ordinal\", OrdinalEncoder(\n",
    "            categories=ordinal_orders,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_transformer, num_cols))\n",
    "if cat_nominal:\n",
    "    transformers.append((\"cat_nom\", cat_nominal_transformer, cat_nominal))\n",
    "if ordinal_cols and cat_ordinal_transformer is not None:\n",
    "    transformers.append((\"cat_ord\", cat_ordinal_transformer, ordinal_cols))\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# --- Modelo: XGBoost o fallback a HistGradientBoosting ---\n",
    "model = None\n",
    "used_xgb = False\n",
    "if TRY_XGBOOST:\n",
    "    try:\n",
    "        from xgboost import XGBRegressor\n",
    "        model = XGBRegressor(**XGB_PARAMS)\n",
    "        used_xgb = True\n",
    "        print(\"Modelo: XGBRegressor\")\n",
    "    except Exception as e:\n",
    "        print(\"XGBoost no disponible o fall√≥ la importaci√≥n; usando HistGradientBoostingRegressor.\", e)\n",
    "\n",
    "if model is None:\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "    model = HistGradientBoostingRegressor(**HGB_PARAMS)\n",
    "    print(\"Modelo: HistGradientBoostingRegressor\")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", model)])\n",
    "\n",
    "# ---------- (Opcional) B√∫squeda autom√°tica r√°pida ----------\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# param_grid = {\n",
    "#     \"model__n_estimators\": [400, 600, 800],\n",
    "#     \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "#     \"model__max_depth\": [4, 6, 8],\n",
    "#     \"model__subsample\": [0.7, 0.8, 0.9],\n",
    "#     \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "#     \"model__reg_lambda\": [0.5, 1.0, 2.0]\n",
    "# }\n",
    "# search = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=12,\n",
    "#                             scoring=\"r2\", cv=3, n_jobs=-1, random_state=42, verbose=1)\n",
    "# search.fit(X_train, y_train)\n",
    "# pipe = search.best_estimator_\n",
    "# print(\"Mejores par√°metros:\", search.best_params_)\n",
    "\n",
    "# Entrenar\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# Invertir transformaci√≥n del target para m√©tricas en escala original\n",
    "def inv_target(z):\n",
    "    return np.expm1(z) if LOG1P_TARGET else z\n",
    "\n",
    "y_val_eval = inv_target(y_val)\n",
    "y_test_eval = inv_target(y_test)\n",
    "y_val_pred_eval = inv_target(y_val_pred)\n",
    "y_test_pred_eval = inv_target(y_test_pred)\n",
    "\n",
    "# M√©tricas + MAPE\n",
    "def report_split(name, y_true, y_hat):\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "    # MAPE evitando divisi√≥n por cero\n",
    "    m = (np.abs((y_true - y_hat)[y_true != 0] / y_true[y_true != 0])).mean() * 100 if np.any(y_true != 0) else np.nan\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE:  {mae:.3f}\")\n",
    "    print(f\"  MSE:  {mse:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  R2:   {r2:.6f}\")\n",
    "    print(f\"  MAPE: {m:.2f}%\")\n",
    "\n",
    "print(\"\\nVALIDACI√ìN - Regresi√≥n:\")\n",
    "report_split(\"VAL\", y_val_eval, y_val_pred_eval)\n",
    "\n",
    "print(\"\\nTEST - Regresi√≥n (evaluaci√≥n final):\")\n",
    "report_split(\"TEST\", y_test_eval, y_test_pred_eval)\n",
    "\n",
    "# --- Importancia por permutaci√≥n (nivel columnas de entrada) ---\n",
    "try:\n",
    "    input_feature_names = []\n",
    "    if num_cols:      input_feature_names += list(num_cols)\n",
    "    if cat_nominal:   input_feature_names += list(cat_nominal)\n",
    "    if ordinal_cols:  input_feature_names += list(ordinal_cols)\n",
    "\n",
    "    result = permutation_importance(\n",
    "        pipe, X_val, y_val, n_repeats=N_PERMUT, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    imp = pd.Series(result.importances_mean, index=input_feature_names).sort_values(ascending=False)\n",
    "    print(f\"\\nTop {min(TOPK_IMP, len(imp))} features por permutaci√≥n (entrada):\")\n",
    "    display(imp.head(TOPK_IMP))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo calcular la importancia por permutaci√≥n:\", e)\n",
    "\n",
    "print(\"\\nNotas:\")\n",
    "print(\"- A√±adidas: is_rush_hour, comfort_index, wind_discomfort, is_holiday_or_weekend.\")\n",
    "print(f\"- Modelo usado: {'XGBRegressor' if used_xgb else 'HistGradientBoostingRegressor'}; LOG1P_TARGET={LOG1P_TARGET}.\")\n",
    "print(\"- Ajusta XGB_PARAMS o activa la b√∫squeda autom√°tica (RandomizedSearchCV) para subir R¬≤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1874d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6f1874d9",
    "outputId": "8645644e-4a24-4677-e601-7d6669a6a671"
   },
   "outputs": [],
   "source": [
    "#@title 10) Diagn√≥stico visual mejorado (ticks limpios, sanitizaci√≥n, zoom y bins)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================\n",
    "# Build DataFrame de evaluaci√≥n\n",
    "# =========================\n",
    "df_eval = pd.DataFrame({\"y_real\": y_test_eval, \"y_pred\": y_test_pred_eval})\n",
    "df_eval[\"residual\"] = df_eval[\"y_real\"] - df_eval[\"y_pred\"]\n",
    "\n",
    "if \"X_test\" in locals():\n",
    "    df_plot = pd.concat([df_eval.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "else:\n",
    "    df_plot = df_eval.copy()\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def yclip(series, q=0.99):\n",
    "    \"\"\"Devuelve un (min, max) sim√©trico por cuantil absoluto para 'zoom'.\"\"\"\n",
    "    lim = series.abs().quantile(q)\n",
    "    lim = float(lim) if np.isfinite(lim) and lim > 0 else float(series.abs().max() or 1.0)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def safe_num(s, lo=None, hi=None, round_to_int=False, allow_na_int=True, clip=True):\n",
    "    \"\"\"Convierte a num√©rico, limpia inf, recorta y opcionalmente redondea/castea a entero con NA.\"\"\"\n",
    "    x = pd.to_numeric(s, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    if clip and (lo is not None or hi is not None):\n",
    "        x = x.clip(lower=lo, upper=hi)\n",
    "    if round_to_int:\n",
    "        x = x.round()\n",
    "        if allow_na_int:\n",
    "            x = x.astype(\"Int64\")  # entero que acepta NA\n",
    "        else:\n",
    "            x = x.fillna(0).astype(int)\n",
    "    return x\n",
    "\n",
    "# =========================\n",
    "# 1) Pred vs Real\n",
    "# =========================\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(data=df_plot, x=\"y_real\", y=\"y_pred\", s=15, alpha=0.6)\n",
    "minv = float(df_plot[[\"y_real\",\"y_pred\"]].min().min())\n",
    "maxv = float(df_plot[[\"y_real\",\"y_pred\"]].max().max())\n",
    "plt.plot([minv, maxv], [minv, maxv], ls=\"--\", color=\"red\", label=\"y = x\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Predicci√≥n vs Real (Test)\")\n",
    "plt.xlabel(\"Real\"); plt.ylabel(\"Predicho\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 2) Distribuci√≥n de residuales (completa y con zoom)\n",
    "# =========================\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_plot[\"residual\"], bins=60, kde=True)\n",
    "plt.title(\"Distribuci√≥n de residuales (y_real - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "low, high = yclip(df_plot[\"residual\"], 0.995)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_plot[\"residual\"].clip(low, high), bins=60, kde=True)\n",
    "plt.title(\"Distribuci√≥n de residuales (zoom 99.5%)\")\n",
    "plt.xlabel(\"Residual (recortado)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 3) Residuales vs Predicci√≥n (zoom 99%)\n",
    "# =========================\n",
    "plt.figure(figsize=(7,4))\n",
    "ylim = yclip(df_plot[\"residual\"], 0.99)\n",
    "sns.scatterplot(data=df_plot, x=\"y_pred\", y=df_plot[\"residual\"].clip(*ylim), s=10, alpha=0.6)\n",
    "plt.axhline(0, ls=\"--\", color=\"red\")\n",
    "plt.ylim(ylim)\n",
    "plt.title(\"Residuales vs Predicci√≥n (zoom 99%)\")\n",
    "plt.xlabel(\"Predicci√≥n\"); plt.ylabel(\"Residual\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 4) Por HORA: boxplot + MAE (con sanitizaci√≥n)\n",
    "# =========================\n",
    "if \"Hour\" in df_plot.columns or \"__Hour\" in df_plot.columns:\n",
    "    hcol = \"Hour\" if \"Hour\" in df_plot.columns else \"__Hour\"\n",
    "    tmp = df_plot.copy()\n",
    "    # Sanitizar a [0,23], entero con NA permitido\n",
    "    tmp[hcol] = safe_num(tmp[hcol], lo=0, hi=23, round_to_int=True, allow_na_int=True, clip=True)\n",
    "    tmp[\"Hour_str\"] = tmp[hcol].astype(str).fillna(\"NA\")\n",
    "\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    sns.boxplot(data=tmp, x=\"Hour_str\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.ylim(ylim); plt.title(\"Residuales por Hour (zoom 99%)\")\n",
    "    plt.xlabel(\"Hour\"); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # MAE por hora (excluye NA)\n",
    "    mae_hour = (\n",
    "        tmp[tmp[\"Hour_str\"] != \"NA\"]\n",
    "        .groupby(hcol)[\"residual\"]\n",
    "        .apply(lambda s: s.abs().mean())\n",
    "        .sort_index()\n",
    "    )\n",
    "    plt.figure(figsize=(12,3))\n",
    "    mae_hour.plot(kind=\"bar\")\n",
    "    plt.title(\"MAE por Hour\")\n",
    "    plt.xlabel(\"Hour\"); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 5) Por WEEKDAY: boxplot + MAE (con sanitizaci√≥n)\n",
    "# =========================\n",
    "if \"Weekday\" in df_plot.columns:\n",
    "    tmp = df_plot.copy()\n",
    "    tmp[\"Weekday\"] = safe_num(tmp[\"Weekday\"], lo=0, hi=6, round_to_int=True, allow_na_int=True, clip=True)\n",
    "    day_labels = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "    # Mapear con NA seguro\n",
    "    mapper = dict(enumerate(day_labels))\n",
    "    tmp[\"Weekday_lbl\"] = tmp[\"Weekday\"].map(mapper).astype(object)\n",
    "    tmp[\"Weekday_lbl\"] = tmp[\"Weekday_lbl\"].fillna(\"NA\")\n",
    "\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "    plt.figure(figsize=(9,4))\n",
    "    sns.boxplot(data=tmp, x=\"Weekday_lbl\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.ylim(ylim); plt.title(\"Residuales por Weekday (zoom 99%)\")\n",
    "    plt.xlabel(\"Weekday\"); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # MAE por weekday (excluye NA)\n",
    "    mae_wd = (\n",
    "        tmp[tmp[\"Weekday_lbl\"] != \"NA\"]\n",
    "        .groupby(\"Weekday_lbl\")[\"residual\"]\n",
    "        .apply(lambda s: s.abs().mean())\n",
    "        .reindex(day_labels)  # asegurar orden Lun..Dom\n",
    "    )\n",
    "    plt.figure(figsize=(9,3))\n",
    "    mae_wd.plot(kind=\"bar\")\n",
    "    plt.title(\"MAE por Weekday\")\n",
    "    plt.xlabel(\"Weekday\"); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 6) Temperatura / Humedad: agrupar en BINS (qcut) y MAE por bin\n",
    "# =========================\n",
    "def boxplot_by_bins(df_in, col, bins=10, label=None):\n",
    "    if col not in df_in.columns:\n",
    "        return\n",
    "    tmp = df_in[[col, \"residual\"]].dropna().copy()\n",
    "    if tmp.empty:\n",
    "        return\n",
    "    # Bins por cuantiles; si hay duplicados de borde, qcut los maneja\n",
    "    try:\n",
    "        tmp[\"bin\"] = pd.qcut(tmp[col], q=bins, duplicates=\"drop\")\n",
    "    except ValueError:\n",
    "        # Si hay muy pocos valores distintos\n",
    "        tmp[\"bin\"] = pd.cut(tmp[col], bins=min(bins, 5))\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    sns.boxplot(data=tmp, x=\"bin\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(ylim); plt.title(f\"Residuales por {label or col} (bins, zoom 99%)\")\n",
    "    plt.xlabel(label or col); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    mae_bin = tmp.groupby(\"bin\")[\"residual\"].apply(lambda s: s.abs().mean())\n",
    "    plt.figure(figsize=(12,3))\n",
    "    mae_bin.plot(kind=\"bar\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(f\"MAE por {label or col} (bins)\")\n",
    "    plt.xlabel(label or col); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "boxplot_by_bins(df_plot, \"Temperature(¬∞C)\", bins=12, label=\"Temperature(¬∞C)\")\n",
    "boxplot_by_bins(df_plot, \"Humidity(%)\",     bins=12, label=\"Humidity(%)\")\n",
    "\n",
    "# =========================\n",
    "# 7) Correlaciones de residuales con num√©ricas\n",
    "# =========================\n",
    "num_cols_eval = df_plot.select_dtypes(include=[np.number]).columns\n",
    "if \"residual\" in num_cols_eval:\n",
    "    corr_resid = df_plot[num_cols_eval].corr()[\"residual\"].sort_values(ascending=False)\n",
    "    print(\"üîπ Correlaciones de residuales con variables num√©ricas (positivas = sobreestimaci√≥n):\")\n",
    "    display(corr_resid.head(15))\n",
    "\n",
    "# =========================\n",
    "# 8) Top outliers para inspecci√≥n\n",
    "# =========================\n",
    "topk = 10\n",
    "print(f\"\\nüîé Top {topk} residuales absolutos (para inspecci√≥n):\")\n",
    "cols_show = [\"y_real\",\"y_pred\",\"residual\"]\n",
    "for c in [\"Date\",\"Hour\",\"__Hour\",\"Weekday\",\"Temperature(¬∞C)\",\"Humidity(%)\",\"Rainfall(mm)\",\"Snowfall (cm)\",\"Holiday\",\"is_weekend\",\"is_holiday_or_weekend\"]:\n",
    "    if c in df_plot.columns:\n",
    "        cols_show.append(c)\n",
    "\n",
    "top_df = (\n",
    "    df_plot.reindex(columns=[c for c in cols_show if c in df_plot.columns])\n",
    "    .iloc[np.argsort(-df_plot['residual'].abs())[:topk]]\n",
    ")\n",
    "display(top_df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
