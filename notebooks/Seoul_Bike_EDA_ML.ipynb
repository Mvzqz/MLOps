{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22de9c3b",
   "metadata": {
    "id": "22de9c3b"
   },
   "source": [
    "# Seoul Bike Sharing ‚Äî EDA + Limpieza (Modified) y Comparaci√≥n vs Original (Referencia) + Baseline ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf7a68",
   "metadata": {
    "id": "bfbf7a68"
   },
   "source": [
    "**Reglas**\n",
    "- Trabajamos y modelamos con **`seoul_bike_sharing_modified.csv`** (principal).\n",
    "- **`seoul_bike_sharing_original.csv`** es **solo referencia** para validar limpieza/distribuci√≥n.\n",
    "- Ambos CSV en el **mismo directorio** que este notebook en Drive. El notebook puede *auto-descubrir* si no das la ruta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c2d29",
   "metadata": {
    "id": "992c2d29"
   },
   "source": [
    "## 1) Setup e imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c468f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3412,
     "status": "ok",
     "timestamp": 1759980255416,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "73c468f5",
    "outputId": "255b1348-a939-436a-b7e3-9de037266e52"
   },
   "outputs": [],
   "source": [
    "#@title Setup e imports\n",
    "import os, sys, warnings, json, hashlib, textwrap, math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('Versions -> numpy', np.__version__, '| pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68bcee",
   "metadata": {
    "id": "8d68bcee"
   },
   "source": [
    "## 2) Ubicaci√≥n de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31848f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24318,
     "status": "ok",
     "timestamp": 1759982913221,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "6e31848f",
    "outputId": "9e285fe3-263d-49b1-eef4-b741eceef0f5"
   },
   "outputs": [],
   "source": [
    "#@title Montar Drive y resolver rutas (mismo directorio del notebook)\n",
    "use_drive = True #@param {type:\"boolean\"}\n",
    "folder_path = \"\" #@param {type:\"string\"}\n",
    "fname_original = \"seoul_bike_sharing_original.csv\"; fname_modified = \"seoul_bike_sharing_modified.csv\"\n",
    "if use_drive:\n",
    "    from google.colab import drive; drive.mount('/content/drive')\n",
    "    base = Path('/content/drive/MyDrive')\n",
    "    if folder_path:\n",
    "        data_dir = Path(folder_path); assert data_dir.exists(), f\"Ruta no encontrada: {data_dir}\"\n",
    "        p_org, p_mod = data_dir/fname_original, data_dir/fname_modified\n",
    "    else:\n",
    "        print(\"üîé Buscando archivos en tu Drive...\")\n",
    "        orgs = list(base.rglob(fname_original)); mods = list(base.rglob(fname_modified))\n",
    "        assert orgs and mods, \"No se encontraron ambos CSV. Especifica 'folder_path'.\"\n",
    "        commons = {p.parent for p in orgs} & {p.parent for p in mods}\n",
    "        if commons:\n",
    "            data_dir = sorted(commons)[0]; p_org, p_mod = data_dir/fname_original, data_dir/fname_modified\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Est√°n en carpetas distintas. Se usar√°n rutas individuales.\"); p_org, p_mod = orgs[0], mods[0]\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"üëâ Sube los dos CSV\"); up = files.upload()\n",
    "    assert 'seoul_bike_sharing_original.csv' in up and 'seoul_bike_sharing_modified.csv' in up\n",
    "    p_org = Path('/content/seoul_bike_sharing_original.csv'); p_mod = Path('/content/seoul_bike_sharing_modified.csv')\n",
    "print(\"Original:\", p_org); print(\"Modified:\", p_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580e305",
   "metadata": {
    "id": "f580e305"
   },
   "source": [
    "## 3) Carga (principal=modified, referencia=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943e63e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1759982987678,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "d943e63e",
    "outputId": "4a0cf440-272a-4b35-88c4-e26c6398903c"
   },
   "outputs": [],
   "source": [
    "#@title Cargar\n",
    "org = pd.read_csv(p_org); mod = pd.read_csv(p_mod)\n",
    "org.head(2), mod.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hx1WcCM0JkTu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1759983211430,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "hx1WcCM0JkTu",
    "outputId": "76d1a0b1-7059-45cd-d49a-1ffb94f3952d"
   },
   "outputs": [],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ApHv1WqGJm_E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1759983296254,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "ApHv1WqGJm_E",
    "outputId": "a5ca517f-e695-4373-a74e-8d61e2dfcf96"
   },
   "outputs": [],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe9c22",
   "metadata": {
    "id": "b6fe9c22"
   },
   "source": [
    "## 4) Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211a812",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1759983299415,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "9211a812"
   },
   "outputs": [],
   "source": [
    "#@title Funciones\n",
    "def normalize_cols(df):\n",
    "    df = df.copy(); df.rename(columns={c: c.strip().replace(\"\\\\xa0\",\" \").replace(\"  \",\" \").strip() for c in df.columns}, inplace=True); return df\n",
    "def add_parsed_date(df):\n",
    "    df = df.copy(); d=[c for c in df.columns if \"date\" in c.lower()]; df[\"__Date\"]=pd.to_datetime(df[d[0]], errors=\"coerce\", dayfirst=True) if d else pd.NaT\n",
    "    h=[c for c in df.columns if \"hour\" in c.lower()]; df[\"__Hour\"]=pd.to_numeric(df[h[0]], errors=\"coerce\") if h else np.nan; return df\n",
    "def guess_target(cols):\n",
    "    for c in cols:\n",
    "        if \"rented\" in c.lower() and \"count\" in c.lower(): return c\n",
    "    return None\n",
    "def clean_df(df, target_col):\n",
    "    df=df.copy()\n",
    "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[c]=df[c].astype(str).str.strip().replace({\"nan\": np.nan})\n",
    "    for c in df.columns:\n",
    "        if c!=target_col and df[c].dtype==object:\n",
    "            num=pd.to_numeric(df[c].str.replace(\",\",\"\").str.replace(\"%\",\"\"), errors=\"coerce\")\n",
    "            if num.notna().sum()>=0.5*len(df): df[c]=num\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        q1,q99=df[c].quantile(0.01), df[c].quantile(0.99)\n",
    "        if pd.notna(q1) and pd.notna(q99) and q99>q1: df[c]=df[c].clip(q1,q99)\n",
    "    return df\n",
    "def detect_categoricals(df, target_col):\n",
    "    cats=list(df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns)\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[c].nunique(dropna=True)<=20 and c!=target_col: cats.append(c)\n",
    "    return sorted([c for c in set(cats) if not c.startswith(\"__\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a473f97",
   "metadata": {
    "id": "7a473f97"
   },
   "source": [
    "## 5) Objetivo y EDA r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd947dfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1759983345431,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "dd947dfe",
    "outputId": "426efd34-3033-46d4-ed70-5c2b4943df40"
   },
   "outputs": [],
   "source": [
    "#@title üéØ Objetivo y EDA\n",
    "\n",
    "# Normalizaci√≥n de columnas y fechas\n",
    "org = normalize_cols(org)\n",
    "mod = normalize_cols(mod)\n",
    "org = add_parsed_date(org)\n",
    "mod = add_parsed_date(mod)\n",
    "\n",
    "# Detecci√≥n autom√°tica de columna objetivo\n",
    "target_col = guess_target(mod.columns) or guess_target(org.columns)\n",
    "if target_col is None:\n",
    "    n = mod.select_dtypes(include=np.number).columns\n",
    "    target_col = n[0] if len(n) > 0 else mod.columns[0]\n",
    "\n",
    "print(\"üîπ Columna objetivo detectada:\", target_col)\n",
    "\n",
    "# --- Funci√≥n de resumen (overview) ---\n",
    "def overview(df, name):\n",
    "    info = pd.DataFrame({\n",
    "        \"col\": df.columns,\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"n_missing\": df.isna().sum().values,\n",
    "        \"pct_missing\": (100 * df.isna().sum() / len(df)).round(2).values,\n",
    "        \"n_unique\": [df[c].nunique(dropna=True) for c in df.columns]\n",
    "    })\n",
    "    display(info.sort_values(\"pct_missing\", ascending=False).style.set_caption(name))\n",
    "\n",
    "# Mostrar res√∫menes de ambos conjuntos\n",
    "overview(mod, \"Modified (pre-limpieza)\")\n",
    "overview(org, \"Original (referencia)\")\n",
    "\n",
    "# --- An√°lisis de la variable objetivo ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Limpieza b√°sica de la columna objetivo (quita $, comas, espacios)\n",
    "s = mod[target_col].astype(str).str.replace(r\"[,\\s\\$\\‚Ç¨]\", \"\", regex=True)\n",
    "num = pd.to_numeric(s, errors=\"coerce\")  # convierte a num√©rico, fuerza NaN si hay texto\n",
    "\n",
    "print(f\"Total de filas: {len(num)}\")\n",
    "print(f\"Valores v√°lidos: {num.notna().sum()}  |  Nulos o no convertibles: {num.isna().sum()}\")\n",
    "\n",
    "# Graficar distribuci√≥n\n",
    "num.dropna().astype(float).plot(kind=\"hist\", bins=30)\n",
    "plt.title(f\"Distribuci√≥n del objetivo (Modified, pre-limpieza): {target_col}\")\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q_6sB5GBGH1x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58645,
     "status": "ok",
     "timestamp": 1759984601533,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "Q_6sB5GBGH1x",
    "outputId": "eca39859-b944-44d8-c899-29d39aa6657f"
   },
   "outputs": [],
   "source": [
    "#@title üîç An√°lisis forense de `mixed_type_col` (¬øconviene eliminarla?)\n",
    "import numpy as np, pandas as pd, re, warnings\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Verificaciones previas ---\n",
    "assert 'mod' in globals(), \"No encuentro el DataFrame `mod` (ejecuta celdas 3-5).\"\n",
    "assert 'target_col' in globals(), \"No encuentro `target_col` (ejecuta Celda 5).\"\n",
    "\n",
    "COL = 'mixed_type_col'\n",
    "if COL not in mod.columns:\n",
    "    raise KeyError(f\"No existe `{COL}` en mod.columns\")\n",
    "\n",
    "print(\"=== 1) Perfil b√°sico de la columna ===\")\n",
    "s = mod[COL]\n",
    "print(\"dtype:\", s.dtype)\n",
    "n = len(s)\n",
    "n_missing = s.isna().sum()\n",
    "n_unique = s.nunique(dropna=True)\n",
    "print(f\"filas={n:,} | n_missing={n_missing:,} ({100*n_missing/n:.2f}%) | n_unique={n_unique:,} ({100*n_unique/n:.2f}% del total)\")\n",
    "\n",
    "types_counts = Counter(type(x).__name__ for x in s)\n",
    "print(\"Tipos de valores observados:\", dict(types_counts))\n",
    "\n",
    "top_vals = pd.Series(s.astype(str)).value_counts(dropna=False).head(12)\n",
    "print(\"\\nTop 12 valores (crudos):\")\n",
    "print(top_vals)\n",
    "\n",
    "print(\"\\n=== 2) ¬øSe puede convertir a num√©rico? ===\")\n",
    "sn = (\n",
    "    s.astype(str)\n",
    "     .str.strip()\n",
    "     .str.replace(r\"[,\\s%]\", \"\", regex=True)\n",
    "     .str.replace(r\"[^\\-\\.\\dEe+]\", \"\", regex=True)\n",
    ")\n",
    "num = pd.to_numeric(sn, errors='coerce')\n",
    "pct_numeric = 100 * num.notna().mean()\n",
    "print(f\"% convertibles a n√∫mero: {pct_numeric:.2f}%\")\n",
    "if num.notna().sum() > 0:\n",
    "    print(\"Resumen (solo convertibles):\")\n",
    "    print(num.dropna().astype(float).describe())\n",
    "\n",
    "print(\"\\n=== 3) Asociaci√≥n con el target ===\")\n",
    "y = pd.to_numeric(mod[target_col], errors='coerce')\n",
    "mask = y.notna()\n",
    "y_valid = y[mask]\n",
    "\n",
    "if num.notna().sum() > 0:\n",
    "    corr = np.corrcoef(num[mask].fillna(num[mask].median()), y_valid)[0,1]\n",
    "    print(f\"Correlaci√≥n num√©rica con {target_col}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"No hay suficientes valores num√©ricos para correlaci√≥n directa.\")\n",
    "\n",
    "if s.nunique(dropna=True) <= 50:\n",
    "    grp = pd.DataFrame({\"col\": s[mask], \"y\": y_valid}).dropna().groupby(\"col\")[\"y\"].mean().sort_values(ascending=False)\n",
    "    print(\"\\nMedia del target por categor√≠a (top 10):\")\n",
    "    print(grp.head(10))\n",
    "\n",
    "print(\"\\n=== 4) ¬øPredice demasiado bien por s√≠ sola? ===\")\n",
    "df_tmp = mod[[COL, target_col]].copy()\n",
    "df_tmp[target_col] = pd.to_numeric(df_tmp[target_col], errors='coerce')\n",
    "df_tmp = df_tmp.dropna(subset=[target_col]).copy()\n",
    "X_one = df_tmp[[COL]]\n",
    "y_one = df_tmp[target_col].astype(float)\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), [COL])\n",
    "])\n",
    "model_one = Pipeline(steps=[\n",
    "    ('enc', preproc),\n",
    "    ('rf', RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_r2 = cross_val_score(model_one, X_one, y_one, scoring='r2', cv=cv)\n",
    "scores_rmse = -cross_val_score(model_one, X_one, y_one, scoring='neg_root_mean_squared_error', cv=cv)\n",
    "\n",
    "print(f\"CV R¬≤ (solo `{COL}`):  mean={scores_r2.mean():.4f} ¬±{scores_r2.std():.4f}\")\n",
    "print(f\"CV RMSE (solo `{COL}`): mean={scores_rmse.mean():.2f} ¬±{scores_rmse.std():.2f}\")\n",
    "\n",
    "print(\"\\n=== 5) Comparaci√≥n con y sin la columna ===\")\n",
    "mod_numish = mod.copy()\n",
    "for c in mod_numish.select_dtypes('object').columns:\n",
    "    coerced = pd.to_numeric(mod_numish[c].astype(str).str.replace(r\"[,\\s%]\", \"\", regex=True), errors='coerce')\n",
    "    if coerced.notna().mean() >= 0.6:\n",
    "        mod_numish[c] = coerced\n",
    "\n",
    "A = mod_numish.dropna(subset=[target_col]).copy()\n",
    "A_y = pd.to_numeric(A[target_col], errors='coerce')\n",
    "A = A.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "A = A.loc[A_y.notna()]\n",
    "A_y = A_y.loc[A_y.notna()].astype(float)\n",
    "\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(A, A_y, test_size=0.2, random_state=42)\n",
    "rf_A = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_A.fit(X_train_A, y_train_A)\n",
    "r2_A = r2_score(y_test_A, rf_A.predict(X_test_A))\n",
    "print(f\"R¬≤ (CON `{COL}`): {r2_A:.4f}\")\n",
    "\n",
    "B = mod_numish.drop(columns=[COL], errors='ignore')\n",
    "B = B.dropna(subset=[target_col]).copy()\n",
    "B_y = pd.to_numeric(B[target_col], errors='coerce')\n",
    "B = B.select_dtypes(include=[np.number]).drop(columns=[target_col], errors='ignore')\n",
    "B = B.loc[B_y.notna()]\n",
    "B_y = B_y.loc[B_y.notna()].astype(float)\n",
    "\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(B, B_y, test_size=0.2, random_state=42)\n",
    "rf_B = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_B.fit(X_train_B, y_train_B)\n",
    "r2_B = r2_score(y_test_B, rf_B.predict(X_test_B))\n",
    "print(f\"R¬≤ (SIN `{COL}`): {r2_B:.4f}\")\n",
    "\n",
    "print(\"\\n=== 6) Se√±ales emp√≠ricas ===\")\n",
    "flags = []\n",
    "if n_unique / n > 0.9:\n",
    "    flags.append(\"cardinalidad_altisima\")\n",
    "if len(types_counts) > 1:\n",
    "    flags.append(\"tipos_mezclados\")\n",
    "if scores_r2.mean() >= 0.2:\n",
    "    flags.append(\"predice_demasiado_bien_sola\")\n",
    "if (r2_A - r2_B > 0.05):\n",
    "    flags.append(\"impacto_excesivo_en_modelo\")\n",
    "\n",
    "print(\"Flags activadas:\", flags if flags else \"ninguna\")\n",
    "\n",
    "print(\"\\n=== 7) Recomendaci√≥n preliminar ===\")\n",
    "if flags:\n",
    "    print(\"‚Üí Sugerencia: ELIMINAR `mixed_type_col` del modelado. Motivos:\")\n",
    "    if \"cardinalidad_altisima\" in flags: print(\"  - Cardinalidad ~√∫nica por fila (parece ID o hash).\")\n",
    "    if \"tipos_mezclados\" in flags: print(\"  - Mezcla de tipos (object con strings/n√∫meros/NaN).\")\n",
    "    if \"predice_demasiado_bien_sola\" in flags: print(\"  - Predice demasiado bien por s√≠ sola (posible fuga de informaci√≥n).\")\n",
    "    if \"impacto_excesivo_en_modelo\" in flags: print(\"  - Aporta ganancia an√≥mala al R¬≤ y alta importancia en el modelo.\")\n",
    "else:\n",
    "    print(\"‚Üí No hay se√±ales fuertes de fuga; mantenla con cautela y valida con CV/temporal splits.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf7374",
   "metadata": {
    "id": "1fbf7374"
   },
   "source": [
    "## 6) Limpieza SOLO en modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481b8ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1759984748935,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "d481b8ee",
    "outputId": "b933781e-a2b9-48bc-aec8-d835139b947a"
   },
   "outputs": [],
   "source": [
    "#@title Limpieza y guardado\n",
    "mod_clean = clean_df(mod, target_col)\n",
    "mod_clean[target_col] = pd.to_numeric(mod_clean[target_col], errors=\"coerce\")\n",
    "\n",
    "# Eliminar columna problem√°tica si existe\n",
    "if \"mixed_type_col\" in mod_clean.columns:\n",
    "    mod_clean.drop(columns=[\"mixed_type_col\"], inplace=True)\n",
    "    print(\"Columna 'mixed_type_col' eliminada.\")\n",
    "\n",
    "# Guardar solo el dataset limpio modificado\n",
    "out_dir = Path(p_mod).parent if 'p_mod' in globals() else Path('.')\n",
    "mod_clean_path = out_dir / 'cleaned_modified.csv'\n",
    "mod_clean.to_csv(mod_clean_path, index=False)\n",
    "\n",
    "print(\"Guardado:\", mod_clean_path)\n",
    "print(\"Tama√±o final ->\", mod_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36f7a8",
   "metadata": {
    "id": "ed36f7a8"
   },
   "source": [
    "## 7) ¬øSe conserva la distribuci√≥n? Comparaci√≥n vs original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae915e11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1759984753016,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "ae915e11",
    "outputId": "e41ebc4b-7b39-4c46-92ce-338e8d1b3f12"
   },
   "outputs": [],
   "source": [
    "#@title Stats lado a lado + KS test (reconstruye org_clean al vuelo)\n",
    "from scipy.stats import ks_2samp\n",
    "import pandas as pd\n",
    "\n",
    "# --- Reconstruir org_clean al vuelo (sin depender de la Celda 6) ---\n",
    "if 'p_org' not in globals():\n",
    "    raise RuntimeError(\"No encuentro 'p_org'. Ejecuta la Celda 2 (montaje/carga de rutas) antes de esta.\")\n",
    "\n",
    "# Cargar original crudo\n",
    "org = pd.read_csv(p_org)\n",
    "\n",
    "# Normalizaci√≥n y fechas (usa tus utilidades de la Celda 4 si existen)\n",
    "if 'normalize_cols' in globals():\n",
    "    org = normalize_cols(org)\n",
    "if 'add_parsed_date' in globals():\n",
    "    org = add_parsed_date(org)\n",
    "\n",
    "# Limpiar con la misma funci√≥n usada en mod\n",
    "if 'clean_df' not in globals():\n",
    "    raise RuntimeError(\"No encuentro 'clean_df'. Ejecuta la Celda 4 (funciones) antes de esta.\")\n",
    "org_clean = clean_df(org, target_col)\n",
    "\n",
    "# Asegurar que el target es num√©rico (por consistencia)\n",
    "org_clean[target_col] = pd.to_numeric(org_clean[target_col], errors=\"coerce\")\n",
    "\n",
    "# --- Comparaci√≥n KS entre mod_clean (ya existente) y org_clean (reconstruido) ---\n",
    "common = [c for c in mod_clean.columns if c in org_clean.columns]\n",
    "rows = []\n",
    "\n",
    "for c in common:\n",
    "    a = pd.to_numeric(mod_clean[c], errors=\"coerce\").dropna()\n",
    "    b = pd.to_numeric(org_clean[c], errors=\"coerce\").dropna()\n",
    "    if len(a) > 50 and len(b) > 50:\n",
    "        stat, p = ks_2samp(a, b)\n",
    "        rows.append({\n",
    "            \"col\": c,\n",
    "            \"mean_mod\": a.mean(),\n",
    "            \"std_mod\": a.std(),\n",
    "            \"p50_mod\": a.median(),\n",
    "            \"mean_org\": b.mean(),\n",
    "            \"std_org\": b.std(),\n",
    "            \"p50_org\": b.median(),\n",
    "            \"ks_stat\": stat,\n",
    "            \"ks_pvalue\": p\n",
    "        })\n",
    "\n",
    "cmp_df = pd.DataFrame(rows).sort_values(\"ks_stat\", ascending=False)\n",
    "\n",
    "if len(cmp_df) > 0:\n",
    "    display(cmp_df.style.set_caption(\n",
    "        \"Comparaci√≥n (modified limpio vs original limpio) ‚Äî KS p>0.05 ‚âà sin cambio estad√≠stico fuerte\"\n",
    "    ))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hubo suficientes columnas num√©ricas con datos en ambos datasets para comparar con KS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ebe56",
   "metadata": {
    "id": "735ebe56"
   },
   "source": [
    "## 8) Baseline ML (solo modified limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ERWKvUk7TSke",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 54853,
     "status": "ok",
     "timestamp": 1759986551830,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "ERWKvUk7TSke",
    "outputId": "72031856-414b-423e-a9e8-f9d722b415fd"
   },
   "outputs": [],
   "source": [
    "#@title 8 BIS) Baseline ML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ------------------ Par√°metros editables ------------------\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "RANDOM_STATE = 42\n",
    "DO_STANDARDIZE = False          # √°rboles no lo necesitan\n",
    "N_ESTIMATORS = 500              # un poco m√°s robusto\n",
    "MAX_DEPTH = 20                  # profundidad moderada\n",
    "MIN_SAMPLES_LEAF = 2            # hoja m√≠nima para reducir overfitting\n",
    "MAX_FEATURES = 'sqrt'           # estrategia com√∫n en RF\n",
    "N_PERMUT = 10\n",
    "TOPK_IMP = 15\n",
    "USE_SEASONS_AS_ORDINAL = False  # Seasons como nominal por ciclo anual\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# --- Chequeos previos ---\n",
    "if 'mod_clean' not in globals():\n",
    "    raise RuntimeError(\"No se encontr√≥ 'mod_clean'. Ejecuta la celda de limpieza primero.\")\n",
    "if 'target_col' not in globals() or target_col not in mod_clean.columns:\n",
    "    raise KeyError(\"No se encontr√≥ 'target_col' o no est√° en mod_clean.columns\")\n",
    "\n",
    "# Copia de trabajo + salvaguarda por si mixed sigue ah√≠\n",
    "df = mod_clean.copy()\n",
    "if \"mixed_type_col\" in df.columns:\n",
    "    df = df.drop(columns=[\"mixed_type_col\"])\n",
    "\n",
    "# Target num√©rico + drop filas sin target\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "# ---------- Feature engineering c√≠clico para Hour ----------\n",
    "# Si existe Hour (o __Hour), convierte a num y crea sin/cos\n",
    "hour_candidates = [c for c in ['Hour', '__Hour'] if c in df.columns]\n",
    "if hour_candidates:\n",
    "    hcol = hour_candidates[0]\n",
    "    df[hcol] = pd.to_numeric(df[hcol], errors='coerce')\n",
    "    # reemplazar outliers/NaN razonablemente\n",
    "    df[hcol] = df[hcol].clip(0, 23)\n",
    "    df['Hour_sin'] = np.sin(2*np.pi*df[hcol]/24.0)\n",
    "    df['Hour_cos'] = np.cos(2*np.pi*df[hcol]/24.0)\n",
    "    # (Opcional) si quieres evitar doble conteo, puedes eliminar la original:\n",
    "    # df.drop(columns=[hcol], inplace=True)\n",
    "\n",
    "y = df[target_col].astype(float)\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Detectar tipos base\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_all  = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# --- Definir ordinales vs nominales ---\n",
    "ordinal_specs_all = []\n",
    "\n",
    "# Seasons -> por defecto NOMINAL (USE_SEASONS_AS_ORDINAL=False)\n",
    "if 'Seasons' in X.columns and USE_SEASONS_AS_ORDINAL:\n",
    "    ordinal_specs_all.append((\"Seasons\", [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]))\n",
    "\n",
    "# Functioning Day: binaria ordenada (si existe)\n",
    "if 'Functioning Day' in X.columns:\n",
    "    ordinal_specs_all.append((\"Functioning Day\", [\"No\", \"Yes\"]))\n",
    "\n",
    "ordinal_cols   = [name for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "ordinal_orders = [order for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "cat_nominal = [c for c in cat_all if c not in ordinal_cols]\n",
    "\n",
    "print(f\"Dataset final: {df.shape} | Num√©ricas: {len(num_cols)} | \"\n",
    "      f\"Cat nominales: {len(cat_nominal)} | Cat ordinales: {len(ordinal_cols)} | Target n: {len(y)}\")\n",
    "\n",
    "# ---------- Estratificaci√≥n (binning si regresi√≥n) ----------\n",
    "is_classification = False\n",
    "if y.nunique() <= 20:\n",
    "    is_classification = True\n",
    "\n",
    "if is_classification:\n",
    "    stratify_labels = y\n",
    "else:\n",
    "    n_bins = min(10, int(np.sqrt(len(y))))\n",
    "    try:\n",
    "        stratify_labels = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "    except Exception:\n",
    "        stratify_labels = pd.cut(y, bins=n_bins)\n",
    "\n",
    "# ---------- Split: test y luego val ----------\n",
    "X_rest, X_test, y_rest, y_test, strat_rest, strat_test = train_test_split(\n",
    "    X, y, stratify_labels, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "val_frac_of_rest = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_rest, y_rest, test_size=val_frac_of_rest, random_state=RANDOM_STATE, stratify=strat_rest\n",
    ")\n",
    "\n",
    "print(f\"Split sizes -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# ---------- Preprocesamiento ----------\n",
    "num_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler() if DO_STANDARDIZE else \"passthrough\")\n",
    "])\n",
    "\n",
    "# OneHot compatible con distintas versiones de sklearn\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn < 1.2\n",
    "\n",
    "cat_nominal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "cat_ordinal_transformer = None\n",
    "if ordinal_cols:\n",
    "    cat_ordinal_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ordinal\", OrdinalEncoder(\n",
    "            categories=ordinal_orders,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_transformer, num_cols))\n",
    "if cat_nominal:\n",
    "    transformers.append((\"cat_nom\", cat_nominal_transformer, cat_nominal))\n",
    "if ordinal_cols and cat_ordinal_transformer is not None:\n",
    "    transformers.append((\"cat_ord\", cat_ordinal_transformer, ordinal_cols))\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# ---------- Modelo ----------\n",
    "if is_classification:\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "else:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=RANDOM_STATE,\n",
    "        min_samples_leaf=MIN_SAMPLES_LEAF, max_features=MAX_FEATURES, n_jobs=-1\n",
    "    )\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", model)])\n",
    "\n",
    "# Entrenar\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# ---------- M√©tricas ----------\n",
    "if is_classification:\n",
    "    print(\"\\nVALIDACI√ìN - Clasificaci√≥n:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    print(\"TEST - Clasificaci√≥n:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Test accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "else:\n",
    "    print(\"\\nVALIDACI√ìN - Regresi√≥n:\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_val, y_val_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_val, y_val_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "    print(\"R2:\", r2_score(y_val, y_val_pred))\n",
    "    print(\"\\nTEST - Regresi√≥n (evaluaci√≥n final):\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_test_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "    print(\"R2:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "# ---------- Importancia por permutaci√≥n (alineada a columnas de ENTRADA) ----------\n",
    "try:\n",
    "    input_feature_names = []\n",
    "    if num_cols:      input_feature_names += list(num_cols)\n",
    "    if cat_nominal:   input_feature_names += list(cat_nominal)\n",
    "    if ordinal_cols:  input_feature_names += list(ordinal_cols)\n",
    "\n",
    "    result = permutation_importance(\n",
    "        pipe, X_val, y_val, n_repeats=N_PERMUT, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    imp = pd.Series(result.importances_mean, index=input_feature_names).sort_values(ascending=False)\n",
    "    print(f\"\\nTop {min(TOPK_IMP, len(imp))} features por permutaci√≥n (nivel columnas de entrada):\")\n",
    "    display(imp.head(TOPK_IMP))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo calcular la importancia por permutaci√≥n:\", e)\n",
    "\n",
    "print(\"\\nNotas:\")\n",
    "print(\"- Seasons se trata como NOMINAL por defecto (switch USE_SEASONS_AS_ORDINAL=False).\")\n",
    "print(\"- Hour ahora incluye Hour_sin/Hour_cos; se puede eliminar la columna Hour original si se dejo.\")\n",
    "print(\"- √Årboles sin escalado; RF con min_samples_leaf y max_features para mejor generalizaci√≥n.\")\n",
    "print(\"- Guarda el pipeline con joblib.dump(pipe, 'best_model.joblib').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ckbpqHeuVAcV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 26012,
     "status": "ok",
     "timestamp": 1759988417094,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "ckbpqHeuVAcV",
    "outputId": "e32b6727-a8ad-45c2-8f37-449f4c6563aa"
   },
   "outputs": [],
   "source": [
    "#@title 9) XGBoost + features temporales/derivadas + LOG1P + MAPE + permutaci√≥n\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ------------------ Par√°metros editables ------------------\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Preprocesamiento\n",
    "DO_STANDARDIZE_NUM = False      # boosting no lo necesita\n",
    "USE_SEASONS_AS_ORDINAL = False  # Seasons como nominal (ciclo anual)\n",
    "USE_HOUR_SIN_COS = True         # mantener Hour_sin / Hour_cos si se generaron antes\n",
    "\n",
    "# Modelo (ajusta aqu√≠)\n",
    "TRY_XGBOOST = True              # si no est√° instalado, fallback a HistGradientBoosting\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=700,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "HGB_PARAMS = dict(\n",
    "    max_depth=None, learning_rate=0.05, max_iter=600, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Target\n",
    "LOG1P_TARGET = True            # activar para estabilizar varianza del target\n",
    "N_PERMUT = 10\n",
    "TOPK_IMP = 15\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# --- Chequeos y base ---\n",
    "if 'mod_clean' not in globals():\n",
    "    raise RuntimeError(\"No se encontr√≥ 'mod_clean'. Ejecuta la celda de limpieza primero.\")\n",
    "if 'target_col' not in globals() or target_col not in mod_clean.columns:\n",
    "    raise KeyError(\"No se encontr√≥ 'target_col' o no est√° en mod_clean.columns\")\n",
    "\n",
    "df = mod_clean.copy()\n",
    "if \"mixed_type_col\" in df.columns:\n",
    "    df = df.drop(columns=[\"mixed_type_col\"])\n",
    "\n",
    "# --- Features temporales desde Date ---\n",
    "def ensure_datetime(s):\n",
    "    return pd.to_datetime(s, errors='coerce')\n",
    "\n",
    "if 'Date' in df.columns:\n",
    "    dt = ensure_datetime(df['Date'])\n",
    "    df['Year']    = dt.dt.year\n",
    "    df['Month']   = dt.dt.month\n",
    "    df['Weekday'] = dt.dt.dayofweek  # 0=Lunes ... 6=Domingo\n",
    "    df['is_weekend'] = (df['Weekday'] >= 5).astype(int)\n",
    "    # Codificaci√≥n c√≠clica Month/Weekday\n",
    "    df['Month_sin']   = np.sin(2*np.pi*(df['Month']-1)/12.0)\n",
    "    df['Month_cos']   = np.cos(2*np.pi*(df['Month']-1)/12.0)\n",
    "    df['Weekday_sin'] = np.sin(2*np.pi*df['Weekday']/7.0)\n",
    "    df['Weekday_cos'] = np.cos(2*np.pi*df['Weekday']/7.0)\n",
    "\n",
    "# Si existen Hour/__Hour y no se crearon sin/cos antes, activarlo aqu√≠\n",
    "if USE_HOUR_SIN_COS:\n",
    "    for hcol in ['Hour', '__Hour']:\n",
    "        if hcol in df.columns:\n",
    "            df[hcol] = pd.to_numeric(df[hcol], errors='coerce').clip(0, 23)\n",
    "            if 'Hour_sin' not in df.columns:\n",
    "                df['Hour_sin'] = np.sin(2*np.pi*df[hcol]/24.0)\n",
    "            if 'Hour_cos' not in df.columns:\n",
    "                df['Hour_cos'] = np.cos(2*np.pi*df[hcol]/24.0)\n",
    "            # (Opcional) evita duplicidad si dejas sin/cos\n",
    "            # df.drop(columns=[hcol], inplace=True)\n",
    "            break\n",
    "\n",
    "# --- Features DERIVADAS √∫tiles ---\n",
    "# Hora pico\n",
    "if 'Hour' in df.columns:\n",
    "    df['is_rush_hour'] = df['Hour'].isin([7,8,9,17,18,19]).astype(int)\n",
    "elif '__Hour' in df.columns:\n",
    "    df['is_rush_hour'] = df['__Hour'].isin([7,8,9,17,18,19]).astype(int)\n",
    "\n",
    "# Comodidad percibida aprox.\n",
    "if {'Temperature(¬∞C)', 'Humidity(%)'}.issubset(df.columns):\n",
    "    df['comfort_index'] = df['Temperature(¬∞C)'] - df['Humidity(%)']/5.0\n",
    "\n",
    "# Disconfort por viento\n",
    "if {'Wind speed (m/s)', 'Humidity(%)'}.issubset(df.columns):\n",
    "    df['wind_discomfort'] = df['Wind speed (m/s)'] * df['Humidity(%)']\n",
    "\n",
    "# Feriado o fin de semana (normaliza Holiday a binaria)\n",
    "if 'is_weekend' in df.columns and 'Holiday' in df.columns:\n",
    "    h = df['Holiday'].astype(str).str.lower().isin(['holiday','yes','1','true'])\n",
    "    df['is_holiday_or_weekend'] = ((df['is_weekend'] == 1) | h).astype(int)\n",
    "\n",
    "# --- Target y split base ---\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "if LOG1P_TARGET:\n",
    "    y_raw = df[target_col].astype(float)\n",
    "    y = np.log1p(y_raw)\n",
    "else:\n",
    "    y = df[target_col].astype(float)\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Detectar tipos\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_all  = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "# Definir ordinales vs nominales\n",
    "ordinal_specs_all = []\n",
    "if 'Seasons' in X.columns and USE_SEASONS_AS_ORDINAL:\n",
    "    ordinal_specs_all.append((\"Seasons\", [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]))\n",
    "if 'Functioning Day' in X.columns:\n",
    "    ordinal_specs_all.append((\"Functioning Day\", [\"No\", \"Yes\"]))\n",
    "\n",
    "ordinal_cols   = [name for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "ordinal_orders = [order for (name, order) in ordinal_specs_all if name in X.columns]\n",
    "cat_nominal = [c for c in cat_all if c not in ordinal_cols]\n",
    "\n",
    "print(f\"Dataset final: {df.shape} | Num√©ricas: {len(num_cols)} | \"\n",
    "      f\"Cat nominales: {len(cat_nominal)} | Cat ordinales: {len(ordinal_cols)} | Target n: {len(y)}\")\n",
    "\n",
    "# Estratificaci√≥n por bins (regresi√≥n)\n",
    "n_bins = min(10, int(np.sqrt(len(y))))\n",
    "try:\n",
    "    stratify_labels = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "except Exception:\n",
    "    stratify_labels = pd.cut(y, bins=n_bins)\n",
    "\n",
    "X_rest, X_test, y_rest, y_test, strat_rest, strat_test = train_test_split(\n",
    "    X, y, stratify_labels, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "val_frac_of_rest = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_rest, y_rest, test_size=val_frac_of_rest, random_state=RANDOM_STATE, stratify=strat_rest\n",
    ")\n",
    "print(f\"Split sizes -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "if DO_STANDARDIZE_NUM:\n",
    "    num_steps.append((\"scaler\", StandardScaler()))\n",
    "num_transformer = Pipeline(steps=num_steps)\n",
    "\n",
    "# OneHot compatible versiones\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # sklearn >= 1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # sklearn < 1.2\n",
    "\n",
    "cat_nominal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe)\n",
    "])\n",
    "\n",
    "cat_ordinal_transformer = None\n",
    "if ordinal_cols:\n",
    "    cat_ordinal_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ordinal\", OrdinalEncoder(\n",
    "            categories=ordinal_orders,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_transformer, num_cols))\n",
    "if cat_nominal:\n",
    "    transformers.append((\"cat_nom\", cat_nominal_transformer, cat_nominal))\n",
    "if ordinal_cols and cat_ordinal_transformer is not None:\n",
    "    transformers.append((\"cat_ord\", cat_ordinal_transformer, ordinal_cols))\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# --- Modelo: XGBoost o fallback a HistGradientBoosting ---\n",
    "model = None\n",
    "used_xgb = False\n",
    "if TRY_XGBOOST:\n",
    "    try:\n",
    "        from xgboost import XGBRegressor\n",
    "        model = XGBRegressor(**XGB_PARAMS)\n",
    "        used_xgb = True\n",
    "        print(\"Modelo: XGBRegressor\")\n",
    "    except Exception as e:\n",
    "        print(\"XGBoost no disponible o fall√≥ la importaci√≥n; usando HistGradientBoostingRegressor.\", e)\n",
    "\n",
    "if model is None:\n",
    "    from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "    model = HistGradientBoostingRegressor(**HGB_PARAMS)\n",
    "    print(\"Modelo: HistGradientBoostingRegressor\")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", model)])\n",
    "\n",
    "# ---------- (Opcional) B√∫squeda autom√°tica r√°pida ----------\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# param_grid = {\n",
    "#     \"model__n_estimators\": [400, 600, 800],\n",
    "#     \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "#     \"model__max_depth\": [4, 6, 8],\n",
    "#     \"model__subsample\": [0.7, 0.8, 0.9],\n",
    "#     \"model__colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "#     \"model__reg_lambda\": [0.5, 1.0, 2.0]\n",
    "# }\n",
    "# search = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=12,\n",
    "#                             scoring=\"r2\", cv=3, n_jobs=-1, random_state=42, verbose=1)\n",
    "# search.fit(X_train, y_train)\n",
    "# pipe = search.best_estimator_\n",
    "# print(\"Mejores par√°metros:\", search.best_params_)\n",
    "\n",
    "# Entrenar\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# Invertir transformaci√≥n del target para m√©tricas en escala original\n",
    "def inv_target(z):\n",
    "    return np.expm1(z) if LOG1P_TARGET else z\n",
    "\n",
    "y_val_eval = inv_target(y_val)\n",
    "y_test_eval = inv_target(y_test)\n",
    "y_val_pred_eval = inv_target(y_val_pred)\n",
    "y_test_pred_eval = inv_target(y_test_pred)\n",
    "\n",
    "# M√©tricas + MAPE\n",
    "def report_split(name, y_true, y_hat):\n",
    "    mae = mean_absolute_error(y_true, y_hat)\n",
    "    mse = mean_squared_error(y_true, y_hat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_hat)\n",
    "\n",
    "    # MAPE evitando divisi√≥n por cero\n",
    "    m = (np.abs((y_true - y_hat)[y_true != 0] / y_true[y_true != 0])).mean() * 100 if np.any(y_true != 0) else np.nan\n",
    "\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE:  {mae:.3f}\")\n",
    "    print(f\"  MSE:  {mse:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  R2:   {r2:.6f}\")\n",
    "    print(f\"  MAPE: {m:.2f}%\")\n",
    "\n",
    "print(\"\\nVALIDACI√ìN - Regresi√≥n:\")\n",
    "report_split(\"VAL\", y_val_eval, y_val_pred_eval)\n",
    "\n",
    "print(\"\\nTEST - Regresi√≥n (evaluaci√≥n final):\")\n",
    "report_split(\"TEST\", y_test_eval, y_test_pred_eval)\n",
    "\n",
    "# --- Importancia por permutaci√≥n (nivel columnas de entrada) ---\n",
    "try:\n",
    "    input_feature_names = []\n",
    "    if num_cols:      input_feature_names += list(num_cols)\n",
    "    if cat_nominal:   input_feature_names += list(cat_nominal)\n",
    "    if ordinal_cols:  input_feature_names += list(ordinal_cols)\n",
    "\n",
    "    result = permutation_importance(\n",
    "        pipe, X_val, y_val, n_repeats=N_PERMUT, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    imp = pd.Series(result.importances_mean, index=input_feature_names).sort_values(ascending=False)\n",
    "    print(f\"\\nTop {min(TOPK_IMP, len(imp))} features por permutaci√≥n (entrada):\")\n",
    "    display(imp.head(TOPK_IMP))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo calcular la importancia por permutaci√≥n:\", e)\n",
    "\n",
    "print(\"\\nNotas:\")\n",
    "print(\"- A√±adidas: is_rush_hour, comfort_index, wind_discomfort, is_holiday_or_weekend.\")\n",
    "print(f\"- Modelo usado: {'XGBRegressor' if used_xgb else 'HistGradientBoostingRegressor'}; LOG1P_TARGET={LOG1P_TARGET}.\")\n",
    "print(\"- Ajusta XGB_PARAMS o activa la b√∫squeda autom√°tica (RandomizedSearchCV) para subir R¬≤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1874d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4464,
     "status": "ok",
     "timestamp": 1759989621252,
     "user": {
      "displayName": "Luis Daniel Castillo Alegr√≠a",
      "userId": "11846615988308146116"
     },
     "user_tz": 360
    },
    "id": "6f1874d9",
    "outputId": "fbfd8191-a999-46ae-faaa-da2712c7e46a"
   },
   "outputs": [],
   "source": [
    "#@title 10) Diagn√≥stico visual mejorado (ticks limpios, sanitizaci√≥n, zoom y bins)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# =========================\n",
    "# Build DataFrame de evaluaci√≥n\n",
    "# =========================\n",
    "df_eval = pd.DataFrame({\"y_real\": y_test_eval, \"y_pred\": y_test_pred_eval})\n",
    "df_eval[\"residual\"] = df_eval[\"y_real\"] - df_eval[\"y_pred\"]\n",
    "\n",
    "if \"X_test\" in locals():\n",
    "    df_plot = pd.concat([df_eval.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "else:\n",
    "    df_plot = df_eval.copy()\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def yclip(series, q=0.99):\n",
    "    \"\"\"Devuelve un (min, max) sim√©trico por cuantil absoluto para 'zoom'.\"\"\"\n",
    "    lim = series.abs().quantile(q)\n",
    "    lim = float(lim) if np.isfinite(lim) and lim > 0 else float(series.abs().max() or 1.0)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def safe_num(s, lo=None, hi=None, round_to_int=False, allow_na_int=True, clip=True):\n",
    "    \"\"\"Convierte a num√©rico, limpia inf, recorta y opcionalmente redondea/castea a entero con NA.\"\"\"\n",
    "    x = pd.to_numeric(s, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    if clip and (lo is not None or hi is not None):\n",
    "        x = x.clip(lower=lo, upper=hi)\n",
    "    if round_to_int:\n",
    "        x = x.round()\n",
    "        if allow_na_int:\n",
    "            x = x.astype(\"Int64\")  # entero que acepta NA\n",
    "        else:\n",
    "            x = x.fillna(0).astype(int)\n",
    "    return x\n",
    "\n",
    "# =========================\n",
    "# 1) Pred vs Real\n",
    "# =========================\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(data=df_plot, x=\"y_real\", y=\"y_pred\", s=15, alpha=0.6)\n",
    "minv = float(df_plot[[\"y_real\",\"y_pred\"]].min().min())\n",
    "maxv = float(df_plot[[\"y_real\",\"y_pred\"]].max().max())\n",
    "plt.plot([minv, maxv], [minv, maxv], ls=\"--\", color=\"red\", label=\"y = x\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Predicci√≥n vs Real (Test)\")\n",
    "plt.xlabel(\"Real\"); plt.ylabel(\"Predicho\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 2) Distribuci√≥n de residuales (completa y con zoom)\n",
    "# =========================\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_plot[\"residual\"], bins=60, kde=True)\n",
    "plt.title(\"Distribuci√≥n de residuales (y_real - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "low, high = yclip(df_plot[\"residual\"], 0.995)\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_plot[\"residual\"].clip(low, high), bins=60, kde=True)\n",
    "plt.title(\"Distribuci√≥n de residuales (zoom 99.5%)\")\n",
    "plt.xlabel(\"Residual (recortado)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 3) Residuales vs Predicci√≥n (zoom 99%)\n",
    "# =========================\n",
    "plt.figure(figsize=(7,4))\n",
    "ylim = yclip(df_plot[\"residual\"], 0.99)\n",
    "sns.scatterplot(data=df_plot, x=\"y_pred\", y=df_plot[\"residual\"].clip(*ylim), s=10, alpha=0.6)\n",
    "plt.axhline(0, ls=\"--\", color=\"red\")\n",
    "plt.ylim(ylim)\n",
    "plt.title(\"Residuales vs Predicci√≥n (zoom 99%)\")\n",
    "plt.xlabel(\"Predicci√≥n\"); plt.ylabel(\"Residual\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 4) Por HORA: boxplot + MAE (con sanitizaci√≥n)\n",
    "# =========================\n",
    "if \"Hour\" in df_plot.columns or \"__Hour\" in df_plot.columns:\n",
    "    hcol = \"Hour\" if \"Hour\" in df_plot.columns else \"__Hour\"\n",
    "    tmp = df_plot.copy()\n",
    "    # Sanitizar a [0,23], entero con NA permitido\n",
    "    tmp[hcol] = safe_num(tmp[hcol], lo=0, hi=23, round_to_int=True, allow_na_int=True, clip=True)\n",
    "    tmp[\"Hour_str\"] = tmp[hcol].astype(str).fillna(\"NA\")\n",
    "\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    sns.boxplot(data=tmp, x=\"Hour_str\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.ylim(ylim); plt.title(\"Residuales por Hour (zoom 99%)\")\n",
    "    plt.xlabel(\"Hour\"); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # MAE por hora (excluye NA)\n",
    "    mae_hour = (\n",
    "        tmp[tmp[\"Hour_str\"] != \"NA\"]\n",
    "        .groupby(hcol)[\"residual\"]\n",
    "        .apply(lambda s: s.abs().mean())\n",
    "        .sort_index()\n",
    "    )\n",
    "    plt.figure(figsize=(12,3))\n",
    "    mae_hour.plot(kind=\"bar\")\n",
    "    plt.title(\"MAE por Hour\")\n",
    "    plt.xlabel(\"Hour\"); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 5) Por WEEKDAY: boxplot + MAE (con sanitizaci√≥n)\n",
    "# =========================\n",
    "if \"Weekday\" in df_plot.columns:\n",
    "    tmp = df_plot.copy()\n",
    "    tmp[\"Weekday\"] = safe_num(tmp[\"Weekday\"], lo=0, hi=6, round_to_int=True, allow_na_int=True, clip=True)\n",
    "    day_labels = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n",
    "    # Mapear con NA seguro\n",
    "    mapper = dict(enumerate(day_labels))\n",
    "    tmp[\"Weekday_lbl\"] = tmp[\"Weekday\"].map(mapper).astype(object)\n",
    "    tmp[\"Weekday_lbl\"] = tmp[\"Weekday_lbl\"].fillna(\"NA\")\n",
    "\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "    plt.figure(figsize=(9,4))\n",
    "    sns.boxplot(data=tmp, x=\"Weekday_lbl\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.ylim(ylim); plt.title(\"Residuales por Weekday (zoom 99%)\")\n",
    "    plt.xlabel(\"Weekday\"); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # MAE por weekday (excluye NA)\n",
    "    mae_wd = (\n",
    "        tmp[tmp[\"Weekday_lbl\"] != \"NA\"]\n",
    "        .groupby(\"Weekday_lbl\")[\"residual\"]\n",
    "        .apply(lambda s: s.abs().mean())\n",
    "        .reindex(day_labels)  # asegurar orden Lun..Dom\n",
    "    )\n",
    "    plt.figure(figsize=(9,3))\n",
    "    mae_wd.plot(kind=\"bar\")\n",
    "    plt.title(\"MAE por Weekday\")\n",
    "    plt.xlabel(\"Weekday\"); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# =========================\n",
    "# 6) Temperatura / Humedad: agrupar en BINS (qcut) y MAE por bin\n",
    "# =========================\n",
    "def boxplot_by_bins(df_in, col, bins=10, label=None):\n",
    "    if col not in df_in.columns:\n",
    "        return\n",
    "    tmp = df_in[[col, \"residual\"]].dropna().copy()\n",
    "    if tmp.empty:\n",
    "        return\n",
    "    # Bins por cuantiles; si hay duplicados de borde, qcut los maneja\n",
    "    try:\n",
    "        tmp[\"bin\"] = pd.qcut(tmp[col], q=bins, duplicates=\"drop\")\n",
    "    except ValueError:\n",
    "        # Si hay muy pocos valores distintos\n",
    "        tmp[\"bin\"] = pd.cut(tmp[col], bins=min(bins, 5))\n",
    "    ylim = yclip(tmp[\"residual\"], 0.99)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    sns.boxplot(data=tmp, x=\"bin\", y=tmp[\"residual\"].clip(*ylim))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(ylim); plt.title(f\"Residuales por {label or col} (bins, zoom 99%)\")\n",
    "    plt.xlabel(label or col); plt.ylabel(\"Residual\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    mae_bin = tmp.groupby(\"bin\")[\"residual\"].apply(lambda s: s.abs().mean())\n",
    "    plt.figure(figsize=(12,3))\n",
    "    mae_bin.plot(kind=\"bar\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(f\"MAE por {label or col} (bins)\")\n",
    "    plt.xlabel(label or col); plt.ylabel(\"MAE\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "boxplot_by_bins(df_plot, \"Temperature(¬∞C)\", bins=12, label=\"Temperature(¬∞C)\")\n",
    "boxplot_by_bins(df_plot, \"Humidity(%)\",     bins=12, label=\"Humidity(%)\")\n",
    "\n",
    "# =========================\n",
    "# 7) Correlaciones de residuales con num√©ricas\n",
    "# =========================\n",
    "num_cols_eval = df_plot.select_dtypes(include=[np.number]).columns\n",
    "if \"residual\" in num_cols_eval:\n",
    "    corr_resid = df_plot[num_cols_eval].corr()[\"residual\"].sort_values(ascending=False)\n",
    "    print(\"üîπ Correlaciones de residuales con variables num√©ricas (positivas = sobreestimaci√≥n):\")\n",
    "    display(corr_resid.head(15))\n",
    "\n",
    "# =========================\n",
    "# 8) Top outliers para inspecci√≥n\n",
    "# =========================\n",
    "topk = 10\n",
    "print(f\"\\nüîé Top {topk} residuales absolutos (para inspecci√≥n):\")\n",
    "cols_show = [\"y_real\",\"y_pred\",\"residual\"]\n",
    "for c in [\"Date\",\"Hour\",\"__Hour\",\"Weekday\",\"Temperature(¬∞C)\",\"Humidity(%)\",\"Rainfall(mm)\",\"Snowfall (cm)\",\"Holiday\",\"is_weekend\",\"is_holiday_or_weekend\"]:\n",
    "    if c in df_plot.columns:\n",
    "        cols_show.append(c)\n",
    "\n",
    "top_df = (\n",
    "    df_plot.reindex(columns=[c for c in cols_show if c in df_plot.columns])\n",
    "    .iloc[np.argsort(-df_plot['residual'].abs())[:topk]]\n",
    ")\n",
    "display(top_df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
