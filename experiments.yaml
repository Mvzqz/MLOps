# Experimento base con el modelo por defecto
baseline_hgb:
  model_name: "hist_gradient_boosting_regressor"
  param_grid:
    model__max_depth: [10, 12]
    model__learning_rate: [0.08, 0.1]
    model__max_iter: [400, 500]
    model__l2_regularization": [0.0, 0.5]

  baseline_rfr:
    model_name: "rand_forest_regressor"
    param_grid:
      model__n_estimators": [500]
      model__random_state": [42]
      model__min_samples_leaf": [2]
      model__max_features": ['sqrt']
      model__n_jobs": [-1]
      model__max_depth: [20]
  
  baseline_xgb:
    model_name: "xg_boost"
    param_grid:
      model__n_estimators": [700]
      model__max_depth: [6]
      model__learning_rate: [0.03]
      model__random_state": [42]
      model__subsample": [0.8]
      model__colsample_bytree": [0.8]
      model__reg_lambda": [1.0]
      model__n_jobs": [-1]
      model__tree_method: ['hist'] 

# Experimento para probar regularizaci√≥n en HGB
hgb_with_l2:
  model_name: "hist_gradient_boosting_regressor"
  param_grid:
    model__learning_rate: [0.08, 0.1]
    model__l2_regularization: [0.0, 0.1, 0.5]
  search_mode: "grid"

# Experimento con SVR (Support Vector Regressor)
svr_experiment:
  model_name: "svr"
  param_grid:
    model__C: [0.1, 1, 10]
    model__kernel: ["linear", "rbf"]
  search_mode: "grid"
