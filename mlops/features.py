"""MÃ³dulo para ingenierÃ­a de caracterÃ­sticas y preprocesamiento del dataset Seoul Bike
Sharing.

Integra los pipelines numÃ©ricos y categÃ³ricos definidos originalmente,
junto con generaciÃ³n de nuevas variables y registro de logs para
trazabilidad.
"""

import logging
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from tqdm import tqdm
import typer

from mlops.config import PROCESSED_DATA_DIR

# Inicializar aplicaciÃ³n y logger


app = typer.Typer()
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(name)

# ----------------------------------------------------------
# CreaciÃ³n de Pipelines de Preprocesamiento
# ---------------------------------------------------------


def crear_pipeline_preprocesamiento(vars_numericas, vars_categoricas):
    """Crea un pipeline de preprocesamiento para variables numÃ©ricas y categÃ³ricas.

    Args:
        vars_numericas (list): Columnas numÃ©ricas.
        vars_categoricas (list): Columnas categÃ³ricas.

    Returns:
        ColumnTransformer: Pipeline completo de preprocesamiento.
    """
    logger.info("âš™ï¸ Creando pipeline de preprocesamiento...")

    # Pipeline numÃ©rico
    num_pipeline = Pipeline(
        [("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]
    )

    # Pipeline categÃ³rico
    cat_pipeline = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("encoder", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    # ComposiciÃ³n general
    preprocessor = ColumnTransformer(
        [
            ("numerico", num_pipeline, vars_numericas),
            ("categorico", cat_pipeline, vars_categoricas),
        ]
    )

    logger.info("âœ… Pipeline de preprocesamiento creado correctamente.")
    return preprocessor


# ----------------------------------------------------------
# GeneraciÃ³n de nuevas caracterÃ­sticas personalizadas
# ----------------------------------------------------------


def agregar_caracteristicas_personalizadas(df: pd.DataFrame) -> pd.DataFrame:
    """Crea nuevas columnas derivadas del dataset base.

    Args:
        df (pd.DataFrame): Dataset original.

    Returns:
        pd.DataFrame: Dataset con nuevas columnas agregadas.
    """
    logger.info("ğŸ§© Agregando caracterÃ­sticas derivadas...")

    if "Temperature(Â°C)" in df.columns and "Humidity(%)" in df.columns:
        df["Temp_Humidity_Interaction"] = df["Temperature(Â°C)"] * df["Humidity(%)"]

    if "Hour" in df.columns:
        df["Is_Peak_Hour"] = df["Hour"].apply(
            lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 20) else 0
        )

    if "Seasons" in df.columns:
        le = LabelEncoder()
        df["Season_Encoded"] = le.fit_transform(df["Seasons"])

    if "Functioning Day" in df.columns:
        df["Weekend_Flag"] = df["Functioning Day"].apply(
            lambda x: 0 if x.strip().lower() == "yes" else 1
        )

    logger.info("âœ… Nuevas caracterÃ­sticas generadas correctamente.")
    return df


# ----------------------------------------------------------
# Aplicar pipeline de transformaciÃ³n
# ----------------------------------------------------------


def transformar_datos(preprocessor, X: pd.DataFrame) -> np.ndarray:
    """Aplica el pipeline de preprocesamiento a los datos de entrada.

    Args:
        preprocessor: Pipeline ajustado.
        X (pd.DataFrame): Datos originales.

    Returns:
        np.ndarray: Datos transformados listos para modelado.
    """
    logger.info("ğŸ”„ Aplicando transformaciÃ³n con el pipeline...")
    X_trans = preprocessor.fit_transform(X)
    logger.info(f"âœ… TransformaciÃ³n completada. Nueva forma: {X_trans.shape}")
    return X_trans


# ----------------------------------------------------------
# EjecuciÃ³n principal del script con Typer
# ----------------------------------------------------------


@app.command()
def main(
    input_path: Path = PROCESSED_DATA_DIR / "processed_data.csv",
    output_path: Path = PROCESSED_DATA_DIR / "features.csv",
):
    """Ejecuta el flujo completo de ingenierÃ­a de caracterÃ­sticas:

    - Lectura del dataset procesado
    - GeneraciÃ³n de nuevas caracterÃ­sticas
    - CreaciÃ³n y aplicaciÃ³n de pipelines
    - ExportaciÃ³n de dataset transformado
    """

    logger.info(f"ğŸ“‚ Leyendo datos desde: {input_path}")
    try:
        df = pd.read_csv(input_path)
    except FileNotFoundError:
        logger.error(f"âŒ No se encontrÃ³ el archivo en {input_path}")
        raise typer.Exit(code=1)

    logger.info("ğŸ” Explorando columnas disponibles...")
    logger.info(f"Columnas detectadas: {list(df.columns)}")

    # Crear nuevas variables
    df = agregar_caracteristicas_personalizadas(df)

    # Identificar variables por tipo
    vars_numericas = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    vars_categoricas = df.select_dtypes(include=["object", "category"]).columns.tolist()

    logger.info(f"ğŸ“Š Variables numÃ©ricas: {vars_numericas}")
    logger.info(f"ğŸ”  Variables categÃ³ricas: {vars_categoricas}")

    # Crear y aplicar pipeline
    preprocessor = crear_pipeline_preprocesamiento(vars_numericas, vars_categoricas)

    with tqdm(total=100, desc="Procesando pipeline") as pbar:
        X_trans = transformar_datos(preprocessor, df)
        pbar.update(100)

    # Convertir a DataFrame y guardar
    features = pd.DataFrame(X_trans)
    features.to_csv(output_path, index=False)

    logger.info(f"ğŸ’¾ Archivo de caracterÃ­sticas guardado en: {output_path}")
    logger.info("ğŸ¯ Flujo de ingenierÃ­a de caracterÃ­sticas completado exitosamente.")


if __name__ == "main":
    app()
